import os
import re
import json
import time
import asyncio
import httpx
from concurrent.futures import ThreadPoolExecutor
from urllib.parse import urlparse, parse_qs, unquote
from playwright.async_api import TimeoutError as PlaywrightTimeoutError
from bs4 import BeautifulSoup
from app.providers.base import BaseProvider
from app.models import ScrapedDataItem, ImageInfo
from app.storage import storage_manager
from typing import Any, List


class WeiboProvider(BaseProvider):
    """
    å¾®åšæœç´¢ç»“æœé¡µé¢çš„çˆ¬è™«å®ç°
    æ”¯æŒï¼š
    1. å¾®åšæœç´¢ç»“æœé¡µé¢ (s.weibo.com/weibo)
    2. è‡ªåŠ¨æå–ç¬¬ä¸€æ¡å¸–å­å†…å®¹
    3. å›¾ç‰‡å’Œè§†é¢‘ä¸‹è½½
    4. ç™»å½•çŠ¶æ€ä¿æŒ
    """
    def __init__(self, url: str, rules: dict, save_images: bool = True, output_format: str = "markdown", 
                 force_save: bool = True):
        super().__init__(url, rules, save_images, output_format, force_save, "weibo")
        self.user_data_dir = "./chrome_user_data"
    
    def _load_saved_cookies(self):
        """åŠ è½½å·²ä¿å­˜çš„ç™»å½•cookies"""
        try:
            cookies_file = os.path.join(self.user_data_dir, "login_data", "weibo_cookies.json")
            if os.path.exists(cookies_file):
                with open(cookies_file, 'r', encoding='utf-8') as f:
                    cookies = json.load(f)
                    print(f"ğŸ“‚ åŠ è½½å·²ä¿å­˜çš„ç™»å½•çŠ¶æ€ï¼Œå…± {len(cookies)} ä¸ªcookies")
                    return cookies
        except Exception as e:
            print(f"âš ï¸ åŠ è½½ç™»å½•çŠ¶æ€å¤±è´¥: {e}")
        return None
    
    def _is_weibo_search_page(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå¾®åšæœç´¢é¡µé¢"""
        return "s.weibo.com/weibo" in self.url
    
    def _extract_search_query(self) -> tuple[str, str]:
        """ä»URLä¸­æå–æœç´¢å…³é”®è¯"""
        search_query = "default"
        try:
            parsed_url = urlparse(self.url)
            query_params = parse_qs(parsed_url.query)
            if 'q' in query_params:
                search_query = unquote(query_params['q'][0])
        except Exception as e:
            print(f"  - è­¦å‘Š: è§£æURLå…³é”®è¯å¤±è´¥: {e}")
        
        # æ¸…æ´—å…³é”®è¯ï¼Œä½¿å…¶å¯ç”¨äºæ–‡ä»¶å
        safe_search_query = re.sub(r'[\\/:*?"<>|]', '_', search_query)[:30]
        return search_query, safe_search_query
    
    def _parse_count(self, text: str) -> str:
        """ä» 'è¯„è®º 123' æˆ– '123' è¿™æ ·çš„å­—ç¬¦ä¸²ä¸­æå–æ•°å­—ï¼Œæ‰¾ä¸åˆ°åˆ™è¿”å› '0'"""
        match = re.search(r'\\d+', text)
        if match:
            return match.group(0)
        return '0'
    
    async def fetch_and_parse(self) -> Any:
        """ä¸»è¦çš„æŠ“å–å’Œè§£ææ–¹æ³•"""
        if not self._is_weibo_search_page():
            print("âš ï¸ å½“å‰URLä¸æ˜¯å¾®åšæœç´¢é¡µé¢ï¼Œå°è¯•é€šç”¨è§£æ...")
            return await self._parse_generic_weibo()
        
        return await self._parse_weibo_search_page()
    
    async def _parse_weibo_search_page(self) -> Any:
        """ä½¿ç”¨Playwrightè§£æå¾®åšæœç´¢é¡µé¢ï¼ˆæ•´åˆtest.pyé€»è¾‘ï¼‰"""
        def _sync_playwright_parse():
            """åŒæ­¥Playwrightè§£æå‡½æ•°"""
            from playwright.sync_api import sync_playwright
            
            print("=" * 50)
            print("ğŸš€ å¼€å§‹æ‰§è¡Œå¾®åšç¬¬ä¸€æ¡å¸–å­æŠ“å–ä»»åŠ¡...")
            print(f"ç›®æ ‡URL: {self.url}")
            print("=" * 50)
            
            # æå–æœç´¢å…³é”®è¯
            search_query, safe_search_query = self._extract_search_query()
            print(f"å·²è§£ææœç´¢å…³é”®è¯ä¸º: {search_query}")
            
            with sync_playwright() as playwright:
                # åˆ›å»ºæŒä¹…åŒ–ä¸Šä¸‹æ–‡ï¼Œä¿æŒç™»å½•çŠ¶æ€
                context = playwright.chromium.launch_persistent_context(
                    self.user_data_dir,
                    headless=True,  # æŠ“å–æ—¶ä½¿ç”¨æ— å¤´æ¨¡å¼  
                    slow_mo=50,
                    user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36',
                    ignore_default_args=['--enable-automation'],
                    args=['--disable-blink-features=AutomationControlled']
                )
                
                page = context.new_page()
                
                # åŠ è½½å·²ä¿å­˜çš„ç™»å½•cookies
                saved_cookies = self._load_saved_cookies()
                if saved_cookies:
                    context.add_cookies(saved_cookies)
                    print("âœ… ç™»å½•çŠ¶æ€å·²åŠ è½½")
                
                try:
                    print("ğŸŒ å¯¼èˆªè‡³ç›®æ ‡é¡µé¢...")
                    page.goto(self.url, timeout=90000, wait_until='load')
                    print("âœ… é¡µé¢åˆæ­¥åŠ è½½å®Œæˆã€‚")
                    
                    print("â³ æ­£åœ¨ç­‰å¾…æœç´¢ç»“æœåŠ è½½...")
                    robust_post_xpath = "(//*[@id='pl_feedlist_index']//div[@class='card-wrap' and .//div[@class='info']])[1]"
                    page.wait_for_selector(f"xpath={robust_post_xpath}", timeout=60000)
                    print("âœ… ç›®æ ‡å¸–å­å…ƒç´ å·²åŠ è½½ã€‚")
                    
                    first_post = page.locator(robust_post_xpath)
                    
                    # æå–å¸–å­ä¿¡æ¯
                    try:
                        author_name = first_post.locator('a.name').first.inner_text(timeout=5000) or "æœªçŸ¥ä½œè€…"
                    except:
                        author_name = "æœªçŸ¥ä½œè€…"
                    
                    # æå–å¸–å­å†…å®¹
                    post_content = ""
                    try:
                        full_content_locator = first_post.locator('p.txt[node-type="feed_list_content_full"]')
                        standard_content_locator = first_post.locator('p.txt[node-type="feed_list_content"]')
                        if full_content_locator.count() > 0:
                            post_content = full_content_locator.inner_text(timeout=5000)
                        elif standard_content_locator.count() > 0:
                            post_content = standard_content_locator.first.inner_text(timeout=5000)
                    except PlaywrightTimeoutError:
                        post_content = "å†…å®¹æå–å¤±è´¥"
                    
                    # æå–äº’åŠ¨æ•°æ®
                    try:
                        action_buttons = first_post.locator('div.card-act > ul > li')
                        reposts_count = self._parse_count(action_buttons.nth(0).inner_text(timeout=5000))
                        comments_count = self._parse_count(action_buttons.nth(1).inner_text(timeout=5000))
                        likes_count = self._parse_count(action_buttons.nth(2).inner_text(timeout=5000))
                    except:
                        reposts_count = comments_count = likes_count = "0"
                    
                    print(f"\\n--- å¸–å­ä¿¡æ¯ ---")
                    print(f"ä½œè€…: {author_name}")
                    print(f"å†…å®¹: {post_content[:100]}...")
                    print(f"è½¬å‘: {reposts_count}, è¯„è®º: {comments_count}, èµ: {likes_count}")
                    print("------------------")
                    
                    # åˆ›å»ºå­˜å‚¨ç»“æ„
                    storage_info = None
                    downloaded_images = []
                    downloaded_videos = []
                    title = f"{search_query} - {author_name}å¸–å­"
                    
                    if self.force_save:
                        # åˆ›å»ºæ ‡é¢˜ï¼ˆç»“åˆæœç´¢è¯å’Œä½œè€…ï¼‰
                        storage_info = storage_manager.create_article_storage(
                            platform=self.platform_name,
                            title=title,
                            url=self.url
                        )
                        
                        # ä¸‹è½½å›¾ç‰‡
                        if self.save_images:
                            downloaded_images = self._sync_download_images(
                                first_post, page, storage_info
                            )
                            
                            # ä¸‹è½½è§†é¢‘
                            downloaded_videos = self._sync_download_videos(
                                first_post, page, storage_info
                            )
                    
                    # ç»„è£…å®Œæ•´å†…å®¹
                    full_content = f"# {search_query} - å¾®åšæœç´¢ç»“æœ\\n\\n"
                    full_content += f"**ä½œè€…**: {author_name}\\n\\n"
                    full_content += f"**å†…å®¹**: {post_content}\\n\\n"
                    full_content += f"**äº’åŠ¨æ•°æ®**:\\n"
                    full_content += f"- è½¬å‘: {reposts_count}\\n"
                    full_content += f"- è¯„è®º: {comments_count}\\n"
                    full_content += f"- ç‚¹èµ: {likes_count}\\n\\n"
                    
                    if downloaded_images:
                        full_content += f"**å›¾ç‰‡ ({len(downloaded_images)}å¼ )**:\\n"
                        for img_path in downloaded_images:
                            img_name = os.path.basename(img_path)
                            full_content += f"![{img_name}]({img_path})\\n"
                        full_content += "\\n"
                    
                    if downloaded_videos:
                        full_content += f"**è§†é¢‘ ({len(downloaded_videos)}ä¸ª)**:\\n"
                        for video_path in downloaded_videos:
                            video_name = os.path.basename(video_path)
                            full_content += f"- [{video_name}]({video_path})\\n"
                        full_content += "\\n"
                    
                    # ä¿å­˜å†…å®¹
                    if storage_info:
                        storage_manager.save_text_content(storage_info, full_content)
                        
                        if self.output_format == "markdown":
                            storage_manager.save_markdown_content(storage_info, full_content, title)
                        
                        # ä¿å­˜å®Œæ•´çš„JSONæ•°æ®
                        timestamp = time.strftime("%Y%m%d_%H%M%S")
                        json_data = {
                            'search_query': search_query,
                            'source_url': self.url,
                            'author': author_name,
                            'content': post_content,
                            'counts': {
                                'reposts': reposts_count,
                                'comments': comments_count,
                                'likes': likes_count
                            },
                            'local_images': downloaded_images,
                            'local_videos': downloaded_videos,
                            'scraped_timestamp': timestamp
                        }
                        
                        json_path = os.path.join(storage_info["article_dir"], "post_data.json")
                        with open(json_path, 'w', encoding='utf-8') as f:
                            json.dump(json_data, f, ensure_ascii=False, indent=4)
                        
                        storage_manager.save_article_index(storage_info, post_content[:200])
                        
                        print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {storage_info['article_dir']}")
                    
                    # è½¬æ¢å›¾ç‰‡å’Œè§†é¢‘è·¯å¾„ä¸ºImageInfoå¯¹è±¡
                    all_media_infos = []
                    for img_path in downloaded_images:
                        all_media_infos.append(ImageInfo(
                            original_url="weibo_image",
                            local_path=img_path,
                            alt_text=os.path.basename(img_path)
                        ))
                    
                    # è§†é¢‘ä¹Ÿä½œä¸º"å›¾ç‰‡"ä¿¡æ¯å¤„ç†ï¼ˆå› ä¸ºæ¨¡å‹é™åˆ¶ï¼‰
                    for video_path in downloaded_videos:
                        all_media_infos.append(ImageInfo(
                            original_url="weibo_video", 
                            local_path=video_path,
                            alt_text=f"è§†é¢‘: {os.path.basename(video_path)}"
                        ))
                    
                    return ScrapedDataItem(
                        title=f"{search_query} - {author_name}",
                        content=full_content,
                        author=author_name,
                        images=all_media_infos,
                        markdown_content=full_content if self.output_format == "markdown" else None,
                        save_directory=storage_info["article_dir"] if storage_info else None
                    )
                
                except Exception as e:
                    print(f"âŒ å¾®åšé¡µé¢è§£æå¤±è´¥: {e}")
                    return None
                finally:
                    print("ğŸ”’ å…³é—­æµè§ˆå™¨...")
                    context.close()
        
        # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåŒæ­¥ä»£ç 
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor() as executor:
            return await loop.run_in_executor(executor, _sync_playwright_parse)
    
    def _sync_download_images(self, first_post, page, storage_info: dict) -> List[str]:
        """åŒæ­¥ä¸‹è½½å›¾ç‰‡"""
        downloaded_images = []
        
        try:
            image_elements = first_post.locator('div.media-piclist img')
            image_count = image_elements.count()
            
            if image_count > 0:
                print(f"ğŸ–¼ï¸ å‘ç° {image_count} å¼ å›¾ç‰‡ï¼Œå¼€å§‹ä¸‹è½½...")
                
                for i in range(image_count):
                    try:
                        img_tag = image_elements.nth(i)
                        img_url = img_tag.get_attribute('src')
                        
                        if not img_url or not img_url.startswith('http'):
                            continue
                        
                        # è·å–é«˜è´¨é‡å›¾ç‰‡URL
                        large_img_url = img_url.replace('/orj360/', '/large/').replace('/thumbnail/', '/large/')
                        
                        img_filename = f"image_{i + 1}.jpg"
                        local_img_path = os.path.join(storage_info["images_dir"], img_filename)
                        
                        response = httpx.get(large_img_url, stream=True, timeout=20)
                        response.raise_for_status()
                        
                        with open(local_img_path, 'wb') as f:
                            for chunk in response.iter_content(chunk_size=8192):
                                f.write(chunk)
                        
                        downloaded_images.append(local_img_path)
                        print(f"  âœ… å›¾ç‰‡å·²ä¸‹è½½: {local_img_path}")
                        
                    except Exception as e:
                        print(f"  âŒ ä¸‹è½½å›¾ç‰‡å¤±è´¥: {e}")
            else:
                print("ğŸ“· æœªå‘ç°å›¾ç‰‡å†…å®¹")
                
        except Exception as e:
            print(f"âŒ å›¾ç‰‡ä¸‹è½½è¿‡ç¨‹å‡ºé”™: {e}")
        
        return downloaded_images
    
    def _sync_download_videos(self, first_post, page, storage_info: dict) -> List[str]:
        """åŒæ­¥ä¸‹è½½è§†é¢‘"""
        downloaded_videos = []
        
        try:
            video_element = first_post.locator('video')
            
            if video_element.count() > 0:
                print("ğŸ¥ å‘ç°è§†é¢‘ï¼Œæ­£åœ¨è§£æé“¾æ¥...")
                
                try:
                    video_url = video_element.first.get_attribute('src', timeout=5000)
                    
                    if video_url:
                        if video_url.startswith('//'):
                            video_url = 'https:' + video_url
                        
                        print(f"âœ… è§†é¢‘é“¾æ¥è§£ææˆåŠŸ: {video_url}")
                        print("â¬‡ï¸ å¼€å§‹ä¸‹è½½è§†é¢‘...")
                        
                        video_file_path = os.path.join(storage_info["attachments_dir"], "video.mp4")
                        
                        response = httpx.get(video_url, stream=True, timeout=300)
                        response.raise_for_status()
                        
                        with open(video_file_path, 'wb') as f:
                            for chunk in response.iter_content(chunk_size=1024*1024):
                                f.write(chunk)
                        
                        downloaded_videos.append(video_file_path)
                        print(f"âœ… è§†é¢‘ä¸‹è½½æˆåŠŸ: {video_file_path}")
                        
                except Exception as e:
                    print(f"  âŒ ä¸‹è½½è§†é¢‘è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            else:
                print("ğŸ¬ æœªå‘ç°è§†é¢‘å†…å®¹")
                
        except Exception as e:
            print(f"âŒ è§†é¢‘ä¸‹è½½è¿‡ç¨‹å‡ºé”™: {e}")
        
        return downloaded_videos
    
    async def _parse_generic_weibo(self) -> Any:
        """é€šç”¨å¾®åšé¡µé¢è§£æï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰"""
        try:
            html = await self._get_html()
            soup = BeautifulSoup(html, 'lxml')
            
            # å°è¯•æå–åŸºæœ¬ä¿¡æ¯
            title = "å¾®åšå†…å®¹"
            content = soup.get_text()[:500] + "..."
            storage_info = None
            
            if self.force_save:
                storage_info = storage_manager.create_article_storage(
                    platform=self.platform_name,
                    title=title,
                    url=self.url
                )
                
                storage_manager.save_text_content(storage_info, content)
                
                if self.output_format == "markdown":
                    storage_manager.save_markdown_content(storage_info, content, title)
                
                storage_manager.save_article_index(storage_info, content[:200])
            
            return ScrapedDataItem(
                title=title,
                content=content,
                author="å¾®åšç”¨æˆ·",
                markdown_content=content if self.output_format == "markdown" else None,
                save_directory=storage_info["article_dir"] if storage_info else None
            )
            
        except Exception as e:
            print(f"âŒ é€šç”¨å¾®åšè§£æå¤±è´¥: {e}")
            return None
