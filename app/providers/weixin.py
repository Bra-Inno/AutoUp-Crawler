import os
import asyncio
import concurrent.futures
from typing import Any, Optional
from bs4 import BeautifulSoup, Tag
from bs4.element import NavigableString
from playwright.sync_api import sync_playwright
import httpx

from app.providers.base import BaseProvider
from app.models import ScrapedDataItem, ImageInfo
from app.file_utils import get_file_extension
from app.config import settings
from app.storage import storage_manager
from loguru import logger


class WeixinMpProvider(BaseProvider):
    """
    å¾®ä¿¡å…¬ä¼—å·æ–‡ç« é¡µé¢çš„çˆ¬è™«å®ç°
    ä½¿ç”¨ Playwright å¤„ç† JavaScript æ¸²æŸ“å’Œåçˆ¬è™«æœºåˆ¶
    æ”¯æŒå›¾ç‰‡ä¸‹è½½å’Œ Markdown è½¬æ¢
    """
    
    def __init__(self, url: str, rules: dict, save_images: bool = True, output_format: str = "markdown", 
                 force_save: bool = True):
        super().__init__(url, rules, save_images, output_format, force_save, "weixin")
        self.storage_info = None
        self.img_counter = {'count': 1}
    
    async def download_image(self, img_url: str, save_dir: str) -> Optional[str]:
        """å¼‚æ­¥ä¸‹è½½å›¾ç‰‡å¹¶ä¿å­˜åˆ°æœ¬åœ°"""
        if not img_url or not img_url.startswith('http'):
            return None
            
        try:
            img_filename = f"image_{self.img_counter['count']}"
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                img_response = await client.get(img_url)
                img_response.raise_for_status()
                
                # æ£€æŸ¥æ–‡ä»¶å¤§å°
                content_length = img_response.headers.get('Content-Length')
                if content_length and int(content_length) > settings.MAX_IMAGE_SIZE:
                    logger.debug(f"  - å›¾ç‰‡è¿‡å¤§ï¼Œè·³è¿‡: {img_url}")
                    return None
                
                content_type = img_response.headers.get('Content-Type')
                ext = get_file_extension(content_type=content_type, url=img_url, content=img_response.content)
                
                img_save_path = os.path.join(save_dir, f"{img_filename}.{ext}")
                with open(img_save_path, 'wb') as f:
                    f.write(img_response.content)
                
                logger.debug(f"  - å›¾ç‰‡å·²ä¸‹è½½: {img_filename}.{ext}")
                self.img_counter['count'] += 1
                return f"{img_filename}.{ext}"
                
        except Exception as e:
            logger.error(f"  - ä¸‹è½½å›¾ç‰‡å¤±è´¥: {img_url}, é”™è¯¯: {e}")
            return None
    
    def convert_tag_to_markdown(self, tag, save_dir: str) -> str:
        """é€’å½’åœ°å°† BeautifulSoup çš„ tag è½¬æ¢æˆ Markdown å­—ç¬¦ä¸²"""
        markdown_str = ""
        
        if tag.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
            level = int(tag.name[1])
            markdown_str = f"{'#' * level} {tag.get_text(strip=True)}\n\n"
        
        elif tag.name in ['p', 'section']:
            for child in tag.children:
                if isinstance(child, NavigableString):
                    markdown_str += str(child)
                elif child.name == 'img':
                    img_src = child.get('data-src') or child.get('src')
                    alt_text = child.get('alt', 'image')
                    if self.save_images and save_dir:
                        # ä½¿ç”¨åŒæ­¥æ–¹å¼å¤„ç†å›¾ç‰‡ä¸‹è½½ï¼ˆåœ¨å¼‚æ­¥ç¯å¢ƒä¸­éœ€è¦ç‰¹æ®Šå¤„ç†ï¼‰
                        loop = asyncio.get_event_loop()
                        img_local_path = loop.run_until_complete(self.download_image(img_src, save_dir))
                        if img_local_path:
                            markdown_str += f"![{alt_text}]({img_local_path})\n"
                    else:
                        markdown_str += f"![{alt_text}]({img_src})\n"
                elif child.name == 'br':
                    markdown_str += '\n'
                else:
                    markdown_str += self.convert_tag_to_markdown(child, save_dir)
            markdown_str += "\n\n"

        elif tag.name == 'blockquote':
            content = tag.get_text(separator='\n', strip=True)
            markdown_str = ''.join([f"> {line}\n" for line in content.split('\n')]) + "\n"

        elif tag.name == 'pre' or (tag.name == 'section' and 'code-snippet__js' in tag.get('class', [])):
            code_content = tag.get_text()
            markdown_str = f"```\n{code_content.strip()}\n```\n\n"
            
        elif tag.name == 'a':
            link_text = tag.get_text(strip=True)
            href = tag.get('href', '')
            markdown_str = f"[{link_text}]({href})"
            
        elif tag.name == 'strong':
            markdown_str = f"**{tag.get_text(strip=True)}**"
            
        else:
            markdown_str = tag.get_text()
            
        return markdown_str

    async def _fallback_parse(self) -> Any:
        """é™çº§æ–¹æ¡ˆï¼šä½¿ç”¨åŸºç¡€ HTTP è¯·æ±‚è§£æ"""
        logger.info("ğŸ”„ ä½¿ç”¨é™çº§æ–¹æ¡ˆï¼šåŸºç¡€ HTTP æŠ“å–")
        try:
            html_content = await self._get_html()
            soup = BeautifulSoup(html_content, 'lxml')
            
            # å°è¯•å¤šç§æ–¹å¼æå–æ ‡é¢˜
            title = None
            title_selectors = [
                '#activity-name',
                '.rich_media_title',
                'h1',
                '[class*="title"]',
                'title'
            ]
            
            for selector in title_selectors:
                try:
                    if selector.startswith('#') or selector.startswith('.'):
                        title_element = soup.select_one(selector)
                    else:
                        title_element = soup.find(selector)
                    
                    if title_element:
                        title = title_element.get_text(strip=True)
                        if title and len(title) > 5:  # ç¡®ä¿æ ‡é¢˜æœ‰æ„ä¹‰
                            logger.debug(f"âœ… ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ°æ ‡é¢˜: {title[:30]}...")
                            break
                except Exception:
                    continue
            
            if not title:
                logger.error("âŒ æ— æ³•æ‰¾åˆ°æœ‰æ•ˆæ ‡é¢˜")
                return None
            
            # å°è¯•å¤šç§æ–¹å¼æå–ä½œè€…
            author = "æœªçŸ¥ä½œè€…"
            author_selectors = [
                '#js_name',
                '.rich_media_meta_text',
                '[class*="author"]',
                '.author'
            ]
            
            for selector in author_selectors:
                try:
                    if selector.startswith('#') or selector.startswith('.'):
                        author_element = soup.select_one(selector)
                    else:
                        author_element = soup.find(selector)
                    
                    if author_element:
                        temp_author = author_element.get_text(strip=True)
                        if temp_author and len(temp_author) < 50:  # åˆç†çš„ä½œè€…åé•¿åº¦
                            author = temp_author
                            logger.debug(f"âœ… ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ°ä½œè€…: {author}")
                            break
                except Exception:
                    continue
            
            # å°è¯•å¤šç§æ–¹å¼æå–å†…å®¹
            content = ""
            content_selectors = [
                '#js_content',
                '.rich_media_content',
                '[class*="content"]',
                'article',
                '.article-content'
            ]
            
            for selector in content_selectors:
                try:
                    if selector.startswith('#') or selector.startswith('.'):
                        content_element = soup.select_one(selector)
                    else:
                        content_element = soup.find(selector)
                    
                    if content_element:
                        content = content_element.get_text(strip=True)
                        if content and len(content) > 100:  # ç¡®ä¿å†…å®¹æœ‰æ„ä¹‰
                            logger.debug(f"âœ… ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ°å†…å®¹ï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")
                            break
                except Exception:
                    continue
            
            if not content:
                # æœ€åå°è¯•ï¼šè·å–æ•´ä¸ªbodyçš„æ–‡æœ¬
                body = soup.find('body')
                if body:
                    content = body.get_text(strip=True)
                    logger.warning(f"âš ï¸ ä½¿ç”¨bodyæ–‡æœ¬ä½œä¸ºå†…å®¹ï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")
            
            if not content:
                logger.error("âŒ æ— æ³•æ‰¾åˆ°æœ‰æ•ˆå†…å®¹")
                return None
            
            logger.info(f"âœ… é™çº§æ–¹æ¡ˆæŠ“å–æˆåŠŸ - æ ‡é¢˜: {title[:30]}..., å†…å®¹é•¿åº¦: {len(content)}")
            
            return ScrapedDataItem(
                title=title,
                author=author,
                content=content,
                markdown_content=None,
                images=[],
                save_directory=None
            )
            
        except Exception as e:
            logger.error(f"âŒ é™çº§æ–¹æ¡ˆå¤±è´¥: {e}")
            # æä¾›è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
            logger.debug(f"   URL: {self.url}")
            logger.error(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
            import traceback
            logger.error(f"   è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return None

    async def fetch_and_parse(self) -> Any:
        """ä½¿ç”¨ Playwright è·å–å’Œè§£æå¾®ä¿¡å…¬ä¼—å·æ–‡ç« ï¼Œå¤±è´¥æ—¶é™çº§åˆ°åŸºç¡€æŠ“å–"""
        try:
            return await self._playwright_parse()
        except Exception as e:
            logger.warning(f"âš ï¸  Playwright æŠ“å–å¤±è´¥: {e}")
            logger.debug("ğŸ”„ å°è¯•é™çº§æ–¹æ¡ˆ...")
            return await self._fallback_parse()

    def _sync_playwright_parse(self) -> dict:
        """åŒæ­¥ç‰ˆæœ¬çš„ Playwright æŠ“å–å®ç°"""
        with sync_playwright() as playwright:
            # ä½¿ç”¨æŒä¹…åŒ–ä¸Šä¸‹æ–‡ï¼Œå‡å°‘åçˆ¬è™«æ£€æµ‹
            try:
                context = playwright.chromium.launch_persistent_context(
                    settings.USER_DATA_DIR,
                    headless=settings.PLAYWRIGHT_HEADLESS,
                    user_agent=settings.USER_AGENT,
                    ignore_default_args=['--enable-automation'],
                    args=['--disable-blink-features=AutomationControlled']
                )
            except Exception as e:
                raise Exception(f"Playwright æµè§ˆå™¨å¯åŠ¨å¤±è´¥: {e}")
            
            page = context.new_page()
            
            try:
                logger.debug(f"ğŸŒ æ­£åœ¨è®¿é—®é¡µé¢: {self.url}")
                page.goto(self.url, timeout=settings.PLAYWRIGHT_TIMEOUT)
                
                # ç­‰å¾…å…³é”®å†…å®¹åŠ è½½
                page.wait_for_selector('#js_content', timeout=60000)
                logger.debug("âœ… é¡µé¢å†…å®¹å·²åŠ è½½ï¼")
                
                html_content = page.content()
                soup = BeautifulSoup(html_content, 'lxml')
                
                # æå–æ ‡é¢˜
                title_element = soup.find(id='activity-name')
                if not title_element:
                    raise ValueError("æ— æ³•åœ¨é¡µé¢ä¸­æ‰¾åˆ°æ ‡é¢˜å…ƒç´ ")
                title = title_element.get_text(strip=True)
                
                # æå–ä½œè€…
                author_element = soup.find(id='js_name')
                author = author_element.get_text(strip=True) if author_element else "æœªçŸ¥ä½œè€…"
                
                # æå–æ­£æ–‡å†…å®¹
                content_element = soup.find(id='js_content')
                if not content_element:
                    raise ValueError("æ— æ³•åœ¨é¡µé¢ä¸­æ‰¾åˆ°æ­£æ–‡å®¹å™¨")
                
                # çº¯æ–‡æœ¬å†…å®¹
                content = content_element.get_text(strip=True)
                
                # åˆ›å»ºå­˜å‚¨ç»“æ„ï¼ˆå¦‚æœå¯ç”¨å¼ºåˆ¶ä¿å­˜æˆ–éœ€è¦ä¿å­˜å›¾ç‰‡/Markdownï¼‰
                storage_info = None
                if self.force_save or self.save_images or self.output_format == "markdown":
                    storage_info = storage_manager.create_article_storage(
                        platform=self.platform_name,
                        title=title,
                        url=self.url,
                        author=author
                    )
                
                # ä¿å­˜çº¯æ–‡æœ¬å†…å®¹
                if storage_info:
                    storage_manager.save_text_content(storage_info, content)
                
                # å¤„ç† Markdown æ ¼å¼
                markdown_content = None
                if isinstance(content_element, Tag):
                    markdown_parts = []
                    
                    for tag in content_element.find_all(recursive=False):
                        md_part = self._sync_convert_tag_to_markdown(tag, storage_info)
                        markdown_parts.append(md_part)
                    
                    markdown_content = "".join(markdown_parts)
                    
                    # ä¿å­˜ Markdown æ–‡ä»¶
                    if storage_info:
                        storage_manager.save_markdown_content(storage_info, markdown_content, title, author)
                
                # æ”¶é›†å›¾ç‰‡ä¿¡æ¯
                images = []
                if isinstance(content_element, Tag):
                    for img in content_element.find_all('img'):
                        if isinstance(img, Tag):
                            img_src_raw = img.get('data-src') or img.get('src')
                            alt_text_raw = img.get('alt', '')
                            
                            # ç¡®ä¿ç±»å‹å®‰å…¨
                            img_src = str(img_src_raw) if img_src_raw else ""
                            alt_text = str(alt_text_raw) if alt_text_raw else ""
                            
                            if img_src:
                                local_path = None
                                if self.save_images and storage_info:
                                    local_path = self._sync_download_and_save_image(img_src, storage_info, alt_text)
                                
                                images.append({
                                    "original_url": img_src,
                                    "local_path": local_path,
                                    "alt_text": alt_text
                                })
                
                # ä¿å­˜æ–‡ç« ç´¢å¼•
                if storage_info:
                    storage_manager.save_article_index(storage_info, content[:200])
                
                return {
                    "title": title,
                    "author": author,
                    "content": content,
                    "markdown_content": markdown_content,
                    "images": images,
                    "save_directory": storage_info["article_dir"] if storage_info else None,
                    "storage_info": storage_info
                }
                
            except Exception as e:
                raise Exception(f"Playwright é¡µé¢å¤„ç†å¤±è´¥: {e}")
                
            finally:
                context.close()
    
    def _sync_download_image(self, img_url: str, save_dir: str) -> Optional[str]:
        """åŒæ­¥ç‰ˆæœ¬çš„å›¾ç‰‡ä¸‹è½½"""
        if not img_url or not img_url.startswith('http'):
            return None
            
        try:
            img_filename = f"image_{self.img_counter['count']}"
            
            import requests
            response = requests.get(img_url, timeout=30)
            response.raise_for_status()
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            content_length = response.headers.get('Content-Length')
            if content_length and int(content_length) > settings.MAX_IMAGE_SIZE:
                logger.debug(f"  - å›¾ç‰‡è¿‡å¤§ï¼Œè·³è¿‡: {img_url}")
                return None
            
            content_type = response.headers.get('Content-Type')
            ext = get_file_extension(content_type=content_type, url=img_url, content=response.content)
            
            img_save_path = os.path.join(save_dir, f"{img_filename}.{ext}")
            with open(img_save_path, 'wb') as f:
                f.write(response.content)
            
            logger.debug(f"  - å›¾ç‰‡å·²ä¸‹è½½: {img_filename}.{ext}")
            self.img_counter['count'] += 1
            return f"{img_filename}.{ext}"
            
        except Exception as e:
            logger.error(f"  - ä¸‹è½½å›¾ç‰‡å¤±è´¥: {img_url}, é”™è¯¯: {e}")
            return None
    
    def _sync_convert_tag_to_markdown(self, tag, storage_info=None) -> str:
        """åŒæ­¥ç‰ˆæœ¬çš„ Markdown è½¬æ¢"""
        markdown_str = ""
        
        if tag.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
            level = int(tag.name[1])
            markdown_str = f"{'#' * level} {tag.get_text(strip=True)}\n\n"
        
        elif tag.name in ['p', 'section']:
            for child in tag.children:
                if isinstance(child, NavigableString):
                    markdown_str += str(child)
                elif child.name == 'img':
                    img_src = child.get('data-src') or child.get('src')
                    alt_text = child.get('alt', 'image')
                    if self.save_images and storage_info:
                        img_local_path = self._sync_download_and_save_image(str(img_src), storage_info, str(alt_text))
                        if img_local_path:
                            # ä½¿ç”¨ç›¸å¯¹è·¯å¾„åœ¨Markdownä¸­å¼•ç”¨å›¾ç‰‡
                            relative_path = f"images/{os.path.basename(img_local_path)}"
                            markdown_str += f"![{alt_text}]({relative_path})\n"
                        else:
                            markdown_str += f"![{alt_text}]({img_src})\n"
                    else:
                        markdown_str += f"![{alt_text}]({img_src})\n"
                elif child.name == 'br':
                    markdown_str += '\n'
                else:
                    markdown_str += self._sync_convert_tag_to_markdown(child, storage_info)
            markdown_str += "\n\n"

        elif tag.name == 'blockquote':
            content = tag.get_text(separator='\n', strip=True)
            markdown_str = ''.join([f"> {line}\n" for line in content.split('\n')]) + "\n"

        elif tag.name == 'pre' or (tag.name == 'section' and 'code-snippet__js' in tag.get('class', [])):
            code_content = tag.get_text()
            markdown_str = f"```\n{code_content.strip()}\n```\n\n"
            
        elif tag.name == 'a':
            link_text = tag.get_text(strip=True)
            href = tag.get('href', '')
            markdown_str = f"[{link_text}]({href})"
            
        elif tag.name == 'strong':
            markdown_str = f"**{tag.get_text(strip=True)}**"
            
        else:
            markdown_str = tag.get_text()
            
        return markdown_str
    
    def _sync_download_and_save_image(self, img_url: str, storage_info: dict, alt_text: str = "") -> Optional[str]:
        """åŒæ­¥ç‰ˆæœ¬çš„å›¾ç‰‡ä¸‹è½½å’Œä¿å­˜ï¼Œä½¿ç”¨å­˜å‚¨ç®¡ç†å™¨"""
        if not img_url or not img_url.startswith('http'):
            return None
            
        try:
            import requests
            response = requests.get(img_url, timeout=30)
            response.raise_for_status()
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            content_length = response.headers.get('Content-Length')
            if content_length and int(content_length) > settings.MAX_IMAGE_SIZE:
                logger.debug(f"  - å›¾ç‰‡è¿‡å¤§ï¼Œè·³è¿‡: {img_url}")
                return None
            
            # ä½¿ç”¨å­˜å‚¨ç®¡ç†å™¨ä¿å­˜å›¾ç‰‡
            image_info = storage_manager.save_image(
                storage_info, 
                response.content, 
                img_url, 
                alt_text, 
                self.img_counter['count']
            )
            
            self.img_counter['count'] += 1
            return image_info["local_path"]
            
        except Exception as e:
            logger.error(f"  - ä¸‹è½½å›¾ç‰‡å¤±è´¥: {img_url}, é”™è¯¯: {e}")
            return None
    
    async def _playwright_parse(self) -> Any:
        """å¼‚æ­¥åŒ…è£…å™¨ï¼Œåœ¨æ‰§è¡Œå™¨ä¸­è¿è¡ŒåŒæ­¥ Playwright"""
        loop = asyncio.get_event_loop()
        
        # ä½¿ç”¨ ThreadPoolExecutor æ¥è¿è¡ŒåŒæ­¥ä»£ç 
        with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
            try:
                result_dict = await loop.run_in_executor(executor, self._sync_playwright_parse)
                
                # è½¬æ¢å› ScrapedDataItem å¯¹è±¡
                images = [ImageInfo(**img_data) for img_data in result_dict["images"]]
                
                return ScrapedDataItem(
                    title=result_dict["title"],
                    author=result_dict["author"],
                    content=result_dict["content"],
                    markdown_content=result_dict["markdown_content"],
                    images=images,
                    save_directory=result_dict["save_directory"]
                )
                
            except Exception as e:
                raise e
