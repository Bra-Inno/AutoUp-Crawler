"""
å°çº¢ä¹¦Provider - ç¬¦åˆé¡¹ç›®ç»Ÿä¸€è§„èŒƒ
æ”¯æŒè·å–å•ä¸ªç¬”è®°ã€æ‰¹é‡æœç´¢ã€ç”¨æˆ·ç¬”è®°åˆ—è¡¨ç­‰åŠŸèƒ½
"""

import asyncio
import json
import os
import time
import random
import httpx
from typing import Dict, Any, Optional, List
from datetime import datetime
from pathlib import Path
from loguru import logger

from .base import BaseProvider
from ..models import ScrapedDataItem
from ..storage import storage_manager
from ..utils.xhs.apis.xhs_pc_apis import XHS_Apis
from ..file_utils import get_file_extension
from ..utils.xhs.xhs_utils.data_util import handle_note_info, norm_str
from ..config import settings


class XiaohongshuProvider(BaseProvider):
    """
    å°çº¢ä¹¦çˆ¬è™« - ç¬¦åˆé¡¹ç›®ç»Ÿä¸€è§„èŒƒ
    
    ç»§æ‰¿BaseProviderï¼Œæä¾›æ ‡å‡†çš„ç¬”è®°è·å–åŠŸèƒ½
    åŒæ—¶ä¿ç•™åŸæœ‰çš„æ‰¹é‡æœç´¢ã€ç”¨æˆ·ç¬”è®°åˆ—è¡¨ç­‰é«˜çº§åŠŸèƒ½
    """
    
    def __init__(
        self, 
        platform_name: str = "xiaohongshu",
        cookies: Optional[str] = None,
        user_agent: Optional[str] = None,
        max_pages: int = 10,
        count_per_page: int = 20,
        delay: float = 2.0,
        max_retries: int = 3,
        base_delay: float = 5.0,
        save_dir: str = "downloads/xiaohongshu"
    ):
        """
        åˆå§‹åŒ–å°çº¢ä¹¦Provider
        
        Args:
            platform_name: å¹³å°åç§°ï¼Œé»˜è®¤"xiaohongshu"
            cookies: å°çº¢ä¹¦cookieså­—ç¬¦ä¸²ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ä»æµè§ˆå™¨æ•°æ®åŠ è½½
            user_agent: User-Agentå­—ç¬¦ä¸²ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ä»æµè§ˆå™¨æ•°æ®åŠ è½½
            max_pages: æ¯ä¸ªç”¨æˆ·æœ€å¤šçˆ¬å–çš„é¡µæ•°ï¼Œé»˜è®¤10
            count_per_page: æ¯é¡µè·å–çš„ç¬”è®°æ•°ï¼Œé»˜è®¤20
            delay: è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤2.0
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤3
            base_delay: é‡è¯•åŸºç¡€å»¶æ—¶ï¼Œé»˜è®¤5.0ç§’
            save_dir: æ•°æ®ä¿å­˜ç›®å½•ï¼Œé»˜è®¤"downloads/xiaohongshu"
        """
        # ä½¿ç”¨è™šæ‹Ÿå‚æ•°åˆå§‹åŒ–BaseProviderï¼Œå°çº¢ä¹¦ä¸ä½¿ç”¨è¿™äº›å‚æ•°
        super().__init__(
            url="", 
            rules={}, 
            save_images=True, 
            output_format="markdown",
            force_save=True,
            platform_name=platform_name
        )
        
        # è‡ªåŠ¨åŠ è½½cookieså’Œuser_agent
        self.cookies = cookies or self._load_saved_cookies()
        self.user_agent = user_agent or self._load_user_agent()
        
        self.max_pages = max_pages
        self.count_per_page = count_per_page
        self.delay = delay
        self.max_retries = max_retries
        self.base_delay = base_delay
        self.save_dir = save_dir
        
        self.xhs_apis = XHS_Apis()
        
        # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
        os.makedirs(save_dir, exist_ok=True)
        
        logger.info(f"åˆå§‹åŒ–å°çº¢ä¹¦Providerå®Œæˆ")
    
    def _load_saved_cookies(self) -> str:
        """ä»æµè§ˆå™¨æ•°æ®åŠ è½½ä¿å­˜çš„cookies"""
        cookie_file = Path(settings.LOGIN_DATA_DIR) / "xiaohongshu_cookies.json"
        if cookie_file.exists():
            try:
                with open(cookie_file, 'r', encoding='utf-8') as f:
                    cookies_data = json.load(f)
                    
                    # å¤„ç†ä¸¤ç§å¯èƒ½çš„æ ¼å¼
                    if isinstance(cookies_data, dict):
                        if "value" in cookies_data and isinstance(cookies_data["value"], list):
                            # æ ¼å¼: {"value": [{"name": "xxx", "value": "yyy"}, ...]}
                            cookie_list = cookies_data["value"]
                            cookies_str = "; ".join([f"{c['name']}={c['value']}" for c in cookie_list])
                            logger.info(f"æˆåŠŸåŠ è½½ä¿å­˜çš„cookiesï¼Œå…±{len(cookie_list)}ä¸ª")
                        else:
                            # æ ¼å¼: {"name1": "value1", "name2": "value2", ...}
                            cookies_str = "; ".join([f"{k}={v}" for k, v in cookies_data.items()])
                            logger.info(f"æˆåŠŸåŠ è½½ä¿å­˜çš„cookiesï¼Œå…±{len(cookies_data)}ä¸ª")
                    elif isinstance(cookies_data, list):
                        # æ ¼å¼: [{"name": "xxx", "value": "yyy"}, ...]
                        cookies_str = "; ".join([f"{c['name']}={c['value']}" for c in cookies_data])
                        logger.info(f"æˆåŠŸåŠ è½½ä¿å­˜çš„cookiesï¼Œå…±{len(cookies_data)}ä¸ª")
                    else:
                        logger.warning(f"æœªçŸ¥çš„cookiesæ ¼å¼: {type(cookies_data)}")
                        return ""
                    
                    return cookies_str
            except Exception as e:
                logger.warning(f"åŠ è½½cookieså¤±è´¥: {e}")
        
        logger.warning("æœªæ‰¾åˆ°ä¿å­˜çš„cookiesï¼Œè¯·å…ˆè¿è¡Œ scripts/save_xiaohongshu_cookies.py")
        return ""
    
    def _load_user_agent(self) -> str:
        
        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„User-Agent
        logger.info("ä½¿ç”¨é…ç½®çš„User-Agent")
        return settings.USER_AGENT
    
    async def fetch_and_parse(self, note_url: Optional[str] = None) -> Optional[ScrapedDataItem]:
        """
        å®ç°BaseProviderè¦æ±‚çš„æ–¹æ³•ï¼šè·å–å¹¶è§£æå•ä¸ªç¬”è®°
        
        è¿™æ˜¯ç¬¦åˆé¡¹ç›®è§„èŒƒçš„ç»Ÿä¸€æ¥å£ï¼Œä½¿ç”¨storage_managerä¿å­˜æ•°æ®
        
        Args:
            note_url: ç¬”è®°URLï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨self.url
            
        Returns:
            ScrapedDataItemå¯¹è±¡ï¼Œå¤±è´¥è¿”å›None
        """
        if note_url is None:
            note_url = self.url
        
        if not note_url:
            logger.error("æœªæä¾›ç¬”è®°URL")
            return None
        
        logger.info(f"å¼€å§‹è·å–ç¬”è®°: {note_url}")
        
        # ä½¿ç”¨åŸæœ‰çš„fetch_noteæ–¹æ³•è·å–ç¬”è®°
        note_data = await self.fetch_note(note_url)
        
        if not note_data:
            logger.error(f"è·å–ç¬”è®°å¤±è´¥: {note_url}")
            return None
        
        # æå–ç¬”è®°è¯¦æƒ…
        detail = self.extract_note_detail(note_data)
        
        # è·å–åŸºæœ¬ä¿¡æ¯
        title = detail.get('title', 'untitled')
        author = detail.get('author_name', 'Unknown')
        content_text = detail.get('content', '')
        
        # åˆ›å»ºMarkdownæ ¼å¼å†…å®¹
        markdown_content = f"""# {title}

**ä½œè€…**: {author}
**å‘å¸ƒæ—¶é—´**: {detail.get('publish_time', 'Unknown')}
**ç‚¹èµæ•°**: {detail.get('likes', 0)}
**æ”¶è—æ•°**: {detail.get('collects', 0)}
**è¯„è®ºæ•°**: {detail.get('comments', 0)}

## æ­£æ–‡

{content_text}

## æ ‡ç­¾

{', '.join(detail.get('topics', []))}

---
åŸæ–‡é“¾æ¥: {note_url}
"""
        
        # ä½¿ç”¨storage_manageråˆ›å»ºå­˜å‚¨ç›®å½•
        storage_info = storage_manager.create_article_storage(
            platform=self.platform_name,
            title=title,
            url=note_url,
            author=author
        )
        
        # ä¿å­˜Markdownå†…å®¹
        storage_manager.save_markdown_content(
            storage_info=storage_info,
            content=markdown_content
        )
        
        # ä¿å­˜çº¯æ–‡æœ¬å†…å®¹
        storage_manager.save_text_content(
            storage_info=storage_info,
            content=content_text
        )
        
        # ä¿å­˜åŸå§‹JSONæ•°æ®
        raw_data_path = os.path.join(storage_info['article_dir'], "raw_data.json")
        with open(raw_data_path, 'w', encoding='utf-8') as f:
            json.dump(note_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ç¬”è®°ä¿å­˜å®Œæˆ: {storage_info['article_dir']}")
        
        # è¿”å›ScrapedDataItem
        return ScrapedDataItem(
            title=title,
            author=author,
            content=content_text,
            markdown_content=markdown_content,
            images=[],  # å°çº¢ä¹¦å›¾ç‰‡å¤„ç†å¯ä»¥åç»­æ‰©å±•
            save_directory=storage_info['article_dir']
        )

    async def fetch_note(
        self, 
        note_url: str, 
        proxies: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """
        è·å–å•ä¸ªç¬”è®°çš„è¯¦ç»†ä¿¡æ¯
        
        Args:
            note_url: ç¬”è®°URL
            proxies: ä»£ç†è®¾ç½®
            
        Returns:
            APIå“åº”æ•°æ®
        """
        note_info = None
        success = False
        msg = "æœªå¼€å§‹å¤„ç†"
        
        for attempt in range(self.max_retries + 1):
            try:
                if attempt > 0:
                    # è®¡ç®—å»¶æ—¶æ—¶é—´ï¼Œæ¯æ¬¡é‡è¯•å¢åŠ å»¶æ—¶
                    delay = self.base_delay * (2 ** (attempt - 1)) + random.uniform(1, 3)
                    logger.info(f'ç¬¬{attempt}æ¬¡é‡è¯•ï¼Œç­‰å¾…{delay:.1f}ç§’...')
                    await asyncio.sleep(delay)
                
                success, msg, response_data = self.xhs_apis.get_note_info(note_url, self.cookies, proxies or {})
                logger.info(f'APIè°ƒç”¨ç»“æœ (å°è¯•{attempt + 1}): success={success}, msg={msg}')
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯é™æµé”™è¯¯
                if response_data and isinstance(response_data, dict):
                    if response_data.get('code') == 300013:  # è®¿é—®é¢‘æ¬¡å¼‚å¸¸
                        logger.warning(f'æ£€æµ‹åˆ°è®¿é—®é¢‘æ¬¡é™åˆ¶ï¼Œç¬¬{attempt + 1}æ¬¡å°è¯•')
                        if attempt < self.max_retries:
                            continue
                        else:
                            success = False
                            msg = 'é‡è¯•æ¬¡æ•°ç”¨å°½ï¼Œä»ç„¶é‡åˆ°è®¿é—®é¢‘æ¬¡é™åˆ¶'
                            break
                
                if success and response_data:
                    # æ£€æŸ¥æ•°æ®ç»“æ„
                    if not isinstance(response_data, dict):
                        success = False  
                        msg = f'APIè¿”å›æ•°æ®ä¸æ˜¯å­—å…¸æ ¼å¼: {type(response_data)}'
                    elif 'data' not in response_data:
                        success = False
                        msg = f'APIè¿”å›æ•°æ®ä¸­æ²¡æœ‰dataå­—æ®µï¼Œå¯ç”¨å­—æ®µ: {list(response_data.keys())}'
                    elif not isinstance(response_data['data'], dict):
                        success = False
                        msg = f'dataå­—æ®µä¸æ˜¯å­—å…¸æ ¼å¼: {type(response_data["data"])}'
                    elif 'items' not in response_data['data']:
                        success = False
                        msg = f'dataä¸­æ²¡æœ‰itemså­—æ®µï¼Œå¯ç”¨å­—æ®µ: {list(response_data["data"].keys())}'
                    elif not isinstance(response_data['data']['items'], list):
                        success = False
                        msg = f'itemsä¸æ˜¯åˆ—è¡¨æ ¼å¼: {type(response_data["data"]["items"])}'
                    elif len(response_data['data']['items']) == 0:
                        success = False
                        msg = 'itemsåˆ—è¡¨ä¸ºç©º'
                    else:
                        note_info = response_data['data']['items'][0]
                        note_info['url'] = note_url
                        note_info = handle_note_info(note_info)
                        logger.info('ç¬”è®°ä¿¡æ¯å¤„ç†æˆåŠŸ')
                        break  # æˆåŠŸå¤„ç†ï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                else:
                    logger.warning(f'APIè°ƒç”¨å¤±è´¥æˆ–è¿”å›æ•°æ®ä¸ºç©º: success={success}')
                    if attempt < self.max_retries:
                        continue
                    
                break  # å¦‚æœä¸éœ€è¦é‡è¯•ï¼Œè·³å‡ºå¾ªç¯
                    
            except Exception as e:
                success = False
                msg = f'å¤„ç†ç¬”è®°æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}'
                logger.error(f'çˆ¬å–ç¬”è®°å¼‚å¸¸ (å°è¯•{attempt + 1}): {e}', exc_info=True)
                if attempt < self.max_retries:
                    continue
                break
            
        logger.info(f'çˆ¬å–ç¬”è®°ä¿¡æ¯å®Œæˆ {note_url}: success={success}, msg={msg}')
        
        if success and note_info:
            return note_info
        else:
            raise Exception(f"è·å–ç¬”è®°å¤±è´¥: {msg}")

    async def fetch_multiple_notes(
        self, 
        note_urls: List[str], 
        proxies: Optional[Dict] = None
    ) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡è·å–å¤šä¸ªç¬”è®°çš„ä¿¡æ¯
        
        Args:
            note_urls: ç¬”è®°URLåˆ—è¡¨
            proxies: ä»£ç†è®¾ç½®
            
        Returns:
            ç¬”è®°ä¿¡æ¯åˆ—è¡¨
        """
        note_list = []
        total_notes = len(note_urls)
        
        print(f"\nå¼€å§‹æ‰¹é‡çˆ¬å– {total_notes} ä¸ªç¬”è®°...")
        print("=" * 60)
        
        for index, note_url in enumerate(note_urls, 1):
            print(f"\n[{index}/{total_notes}] æ­£åœ¨çˆ¬å–ç¬”è®°: {note_url[-20:]}...")
            
            try:
                note_info = await self.fetch_note(note_url, proxies)
                note_list.append(note_info)
                print(f"âœ“ ç¬”è®°çˆ¬å–å®Œæˆ: {note_info.get('title', 'æœªçŸ¥æ ‡é¢˜')[:30]}...")
                
            except Exception as e:
                print(f"âœ— ç¬”è®°çˆ¬å–å¤±è´¥: {e}")
            
            # ç¬”è®°ä¹‹é—´çš„å»¶è¿Ÿ
            if index < total_notes:
                print(f"ç­‰å¾… {self.delay} ç§’åç»§ç»­ä¸‹ä¸€ä¸ªç¬”è®°...")
                await asyncio.sleep(self.delay)
        
        # ç»Ÿè®¡ä¿¡æ¯
        print("\n" + "=" * 60)
        print("æ‰¹é‡çˆ¬å–å®Œæˆ!")
        print(f"æˆåŠŸ: {len(note_list)} ä¸ªç¬”è®°")
        print(f"å¤±è´¥: {total_notes - len(note_list)} ä¸ªç¬”è®°")
        print("=" * 60 + "\n")
        
        return note_list

    async def fetch_all_user_notes(
        self, 
        user_url: str, 
        max_notes: Optional[int] = None,
        proxies: Optional[Dict] = None
    ) -> List[Dict[str, Any]]:
        """
        è·å–ä¸€ä¸ªç”¨æˆ·çš„æ‰€æœ‰ç¬”è®°
        
        Args:
            user_url: ç”¨æˆ·ä¸»é¡µURL
            max_notes: æœ€å¤šè·å–çš„ç¬”è®°æ•°ï¼ŒNoneè¡¨ç¤ºä¸é™åˆ¶
            proxies: ä»£ç†è®¾ç½®
            
        Returns:
            ç¬”è®°åˆ—è¡¨
        """
        note_list = []
        
        print(f"å¼€å§‹çˆ¬å–ç”¨æˆ·çš„æ‰€æœ‰ç¬”è®°: {user_url}")
        
        try:
            success, msg, all_note_info = self.xhs_apis.get_user_all_notes(user_url, self.cookies, proxies or {})
            
            if not success:
                # å¦‚æœæ˜¯JavaScriptç›¸å…³çš„é”™è¯¯æˆ–è€…è§£æé”™è¯¯ï¼Œè¿”å›ç©ºç»“æœ
                if ("list index out of range" in msg or 
                    "Cannot find module" in msg or 
                    "js" in msg.lower()):
                    logger.warning(f'ç”¨æˆ·ç¬”è®°è·å–åŠŸèƒ½å—é™: {msg}')
                    print(f"âš ï¸ ç”¨æˆ·ç¬”è®°è·å–åŠŸèƒ½æš‚æ—¶å—é™ï¼Œè¿”å›ç©ºç»“æœ")
                    return []
                else:
                    raise Exception(f"è·å–ç”¨æˆ·ç¬”è®°åˆ—è¡¨å¤±è´¥: {msg}")
            
            logger.info(f'ç”¨æˆ· {user_url} ä½œå“æ•°é‡: {len(all_note_info)}')
            
            # æ„å»ºç¬”è®°URLåˆ—è¡¨
            note_urls = []
            for simple_note_info in all_note_info:
                note_url = f"https://www.xiaohongshu.com/explore/{simple_note_info['note_id']}?xsec_token={simple_note_info['xsec_token']}"
                note_urls.append(note_url)
            
            # å¦‚æœè®¾ç½®äº†æœ€å¤§ç¬”è®°æ•°ï¼Œæˆªå–
            if max_notes and len(note_urls) > max_notes:
                note_urls = note_urls[:max_notes]
                print(f"é™åˆ¶è·å–æ•°é‡ä¸º {max_notes} ä¸ªç¬”è®°")
            
            # æ‰¹é‡è·å–ç¬”è®°è¯¦æƒ…
            note_data = await self.fetch_multiple_notes(note_urls, proxies)
            
            print(f"ç”¨æˆ·ç¬”è®°çˆ¬å–å®Œæˆï¼Œå…±è·å– {len(note_data)} ä¸ªç¬”è®°")
            return note_data
            
        except Exception as e:
            logger.error(f'çˆ¬å–ç”¨æˆ·æ‰€æœ‰ç¬”è®°å¤±è´¥ {user_url}: {e}')
            raise
    
    async def fetch_multiple_users_notes(
        self,
        user_urls: List[str],
        max_notes_per_user: Optional[int] = None,
        proxies: Optional[Dict] = None
    ) -> Dict[str, List[Dict[str, Any]]]:
        """
        è·å–å¤šä¸ªç”¨æˆ·çš„æ‰€æœ‰ç¬”è®°
        
        Args:
            user_urls: ç”¨æˆ·URLåˆ—è¡¨
            max_notes_per_user: æ¯ä¸ªç”¨æˆ·æœ€å¤šè·å–çš„ç¬”è®°æ•°ï¼ŒNoneè¡¨ç¤ºä¸é™åˆ¶
            proxies: ä»£ç†è®¾ç½®
            
        Returns:
            å­—å…¸ï¼Œkeyä¸ºç”¨æˆ·URLï¼Œvalueä¸ºç¬”è®°åˆ—è¡¨
        """
        results = {}
        total_users = len(user_urls)
        
        print(f"\nå¼€å§‹æ‰¹é‡çˆ¬å– {total_users} ä¸ªç”¨æˆ·çš„ç¬”è®°...")
        print("=" * 60)
        
        for index, user_url in enumerate(user_urls, 1):
            print(f"\n[{index}/{total_users}] æ­£åœ¨çˆ¬å–ç”¨æˆ·: {user_url[-30:]}...")
            
            try:
                notes = await self.fetch_all_user_notes(
                    user_url=user_url,
                    max_notes=max_notes_per_user,
                    proxies=proxies
                )
                results[user_url] = notes
                print(f"âœ“ ç”¨æˆ· {user_url[-30:]} çˆ¬å–å®Œæˆ: {len(notes)} ä¸ªç¬”è®°")
                
            except Exception as e:
                print(f"âœ— ç”¨æˆ· {user_url[-30:]} çˆ¬å–å¤±è´¥: {e}")
                results[user_url] = []
            
            # ç”¨æˆ·ä¹‹é—´çš„å»¶è¿Ÿ
            if index < total_users:
                print(f"ç­‰å¾… {self.delay} ç§’åç»§ç»­ä¸‹ä¸€ä¸ªç”¨æˆ·...")
                await asyncio.sleep(self.delay)
        
        # ç»Ÿè®¡ä¿¡æ¯
        print("\n" + "=" * 60)
        print("æ‰¹é‡çˆ¬å–å®Œæˆ!")
        print(f"æˆåŠŸ: {sum(1 for notes in results.values() if notes)} ä¸ªç”¨æˆ·")
        print(f"å¤±è´¥: {sum(1 for notes in results.values() if not notes)} ä¸ªç”¨æˆ·")
        print(f"æ€»ç¬”è®°æ•°: {sum(len(notes) for notes in results.values())} ä¸ª")
        print("=" * 60 + "\n")
        
        return results

    async def search_notes(
        self, 
        query: str, 
        require_num: int,
        sort_type_choice: int = 0,
        note_type: int = 0,
        note_time: int = 0,
        note_range: int = 0,
        pos_distance: int = 0,
        geo: Optional[Dict] = None,
        proxies: Optional[Dict] = None
    ) -> List[Dict[str, Any]]:
        """
        æœç´¢æŒ‡å®šæ•°é‡çš„ç¬”è®°
        
        Args:
            query: æœç´¢å…³é”®è¯
            require_num: æœç´¢æ•°é‡
            sort_type_choice: æ’åºæ–¹å¼ 0 ç»¼åˆæ’åº, 1 æœ€æ–°, 2 æœ€å¤šç‚¹èµ, 3 æœ€å¤šè¯„è®º, 4 æœ€å¤šæ”¶è—
            note_type: ç¬”è®°ç±»å‹ 0 ä¸é™, 1 è§†é¢‘ç¬”è®°, 2 æ™®é€šç¬”è®°
            note_time: ç¬”è®°æ—¶é—´ 0 ä¸é™, 1 ä¸€å¤©å†…, 2 ä¸€å‘¨å†…å¤©, 3 åŠå¹´å†…
            note_range: ç¬”è®°èŒƒå›´ 0 ä¸é™, 1 å·²çœ‹è¿‡, 2 æœªçœ‹è¿‡, 3 å·²å…³æ³¨
            pos_distance: ä½ç½®è·ç¦» 0 ä¸é™, 1 åŒåŸ, 2 é™„è¿‘ æŒ‡å®šè¿™ä¸ªå¿…é¡»è¦æŒ‡å®š geo
            geo: åœ°ç†ä½ç½®ä¿¡æ¯
            proxies: ä»£ç†è®¾ç½®
            
        Returns:
            æœç´¢åˆ°çš„ç¬”è®°åˆ—è¡¨
        """
        print(f"å¼€å§‹æœç´¢å…³é”®è¯: {query}ï¼Œç›®æ ‡æ•°é‡: {require_num}")
        
        try:
            success, msg, notes = self.xhs_apis.search_some_note(
                query, require_num, self.cookies, sort_type_choice, 
                note_type, note_time, note_range, pos_distance, 
                str(geo) if geo else "", proxies or {}
            )
            
            if not success:
                # å¦‚æœæ˜¯JavaScriptç›¸å…³çš„é”™è¯¯ï¼Œè¿”å›ç©ºç»“æœè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
                if "Cannot find module" in msg or "js" in msg.lower():
                    logger.warning(f'æœç´¢åŠŸèƒ½å› JavaScripté—®é¢˜è¢«ç¦ç”¨: {msg}')
                    print(f"âš ï¸ æœç´¢åŠŸèƒ½æš‚æ—¶ä¸å¯ç”¨ï¼ˆJavaScriptä¾èµ–é—®é¢˜ï¼‰ï¼Œè¿”å›ç©ºç»“æœ")
                    return []
                else:
                    raise Exception(f"æœç´¢å¤±è´¥: {msg}")
            
            # è¿‡æ»¤å‡ºç¬”è®°ç±»å‹
            notes = list(filter(lambda x: x['model_type'] == "note", notes))
            logger.info(f'æœç´¢å…³é”®è¯ {query} ç¬”è®°æ•°é‡: {len(notes)}')
            
            # æ„å»ºç¬”è®°URLåˆ—è¡¨
            note_urls = []
            for note in notes:
                note_url = f"https://www.xiaohongshu.com/explore/{note['id']}?xsec_token={note['xsec_token']}"
                note_urls.append(note_url)
            
            # æ‰¹é‡è·å–ç¬”è®°è¯¦æƒ…
            note_data = await self.fetch_multiple_notes(note_urls, proxies)
            
            print(f"æœç´¢å®Œæˆï¼Œå…±è·å– {len(note_data)} ä¸ªç¬”è®°")
            return note_data
            
        except Exception as e:
            logger.error(f'æœç´¢å…³é”®è¯ {query} å¤±è´¥: {e}')
            raise
    
    @staticmethod
    def extract_note_detail(note: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä»åŸå§‹ç¬”è®°æ•°æ®ä¸­æå–è¯¦ç»†ä¿¡æ¯
        
        Args:
            note: åŸå§‹ç¬”è®°æ•°æ®å­—å…¸
            
        Returns:
            æå–åçš„ç»“æ„åŒ–ç¬”è®°è¯¦æƒ…
        """
        # åŸºæœ¬ä¿¡æ¯
        note_id = note.get('note_id', '')
        title = note.get('title', '')
        desc = note.get('desc', '')
        time_str = note.get('time', '')
        note_url = note.get('url', '')
        
        # ç»Ÿè®¡æ•°æ®
        interact_info = note.get('interact_info', {})
        statistics = {
            'liked_count': interact_info.get('liked_count', 0),      # ç‚¹èµæ•°
            'collected_count': interact_info.get('collected_count', 0), # æ”¶è—æ•°
            'comment_count': interact_info.get('comment_count', 0),   # è¯„è®ºæ•°
            'share_count': interact_info.get('share_count', 0),       # åˆ†äº«æ•°
        }
        
        # ä½œè€…ä¿¡æ¯
        user_info = note.get('user', {})
        author_info = {
            'user_id': user_info.get('user_id', ''),
            'nickname': user_info.get('nickname', ''),
            'avatar': user_info.get('avatar', ''),
            'desc': user_info.get('desc', ''),
        }
        
        # å†…å®¹ä¿¡æ¯
        content_info = {
            'type': note.get('type', ''),  # ç¬”è®°ç±»å‹ï¼šnormal/video
            'cover': note.get('cover', {}).get('url_default', '') if note.get('cover') else '',
            'images': [],
            'video_url': '',
        }
        
        # å¤„ç†å›¾ç‰‡
        image_list = note.get('image_list', [])
        for img in image_list:
            if isinstance(img, dict) and 'url_default' in img:
                content_info['images'].append(img['url_default'])
        
        # å¤„ç†è§†é¢‘
        video_info = note.get('video', {})
        if video_info:
            media = video_info.get('media', {})
            if media and 'stream' in media:
                stream = media['stream']
                if isinstance(stream, dict) and 'h264' in stream:
                    h264_list = stream['h264']
                    if h264_list and len(h264_list) > 0:
                        content_info['video_url'] = h264_list[0].get('master_url', '')
        
        # è¯é¢˜æ ‡ç­¾
        tag_list = note.get('tag_list', [])
        hashtags = []
        for tag in tag_list:
            if isinstance(tag, dict) and 'name' in tag:
                hashtags.append({
                    'id': tag.get('id', ''),
                    'name': tag.get('name', ''),
                    'type': tag.get('type', '')
                })
        
        return {
            'note_id': note_id,
            'title': title,
            'desc': desc,
            'time': time_str,
            'url': note_url,
            'statistics': statistics,
            'author': author_info,
            'content': content_info,
            'hashtags': hashtags,
            'link': f"https://www.xiaohongshu.com/explore/{note_id}",
            'raw_data': note  # ä¿ç•™åŸå§‹æ•°æ®ä»¥å¤‡éœ€è¦
        }
    
    @staticmethod
    def extract_note_details_batch(note_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡æå–ç¬”è®°è¯¦ç»†ä¿¡æ¯
        
        Args:
            note_list: ç¬”è®°åˆ—è¡¨
            
        Returns:
            æå–åçš„ç¬”è®°è¯¦æƒ…åˆ—è¡¨
        """
        return [XiaohongshuProvider.extract_note_detail(note) for note in note_list]
    
    @staticmethod
    def print_note_detail(detail: Dict[str, Any], index: int = 1):
        """
        æ‰“å°ç¬”è®°è¯¦ç»†ä¿¡æ¯
        
        Args:
            detail: ç¬”è®°è¯¦æƒ…å­—å…¸
            index: åºå·
        """
        print(f"\n{'='*70}")
        print(f"ç¬”è®° {index}: {detail['note_id']}")
        print(f"{'='*70}")
        
        print(f"\nğŸ“ åŸºæœ¬ä¿¡æ¯:")
        print(f"  æ ‡é¢˜: {detail['title']}")
        print(f"  æè¿°: {detail['desc'][:100]}{'...' if len(detail['desc']) > 100 else ''}")
        print(f"  å‘å¸ƒæ—¶é—´: {detail['time']}")
        print(f"  é“¾æ¥: {detail['link']}")
        
        print(f"\nğŸ“Š ç»Ÿè®¡æ•°æ®:")
        stats = detail['statistics']
        print(f"  ğŸ‘ ç‚¹èµ: {stats['liked_count']:,}")
        print(f"  â­ æ”¶è—: {stats['collected_count']:,}")
        print(f"  ğŸ’¬ è¯„è®º: {stats['comment_count']:,}")
        print(f"  ğŸ”— åˆ†äº«: {stats['share_count']:,}")
        
        print(f"\nğŸ¬ å†…å®¹ä¿¡æ¯:")
        content = detail['content']
        print(f"  ç±»å‹: {content['type']}")
        if content['images']:
            print(f"  å›¾ç‰‡æ•°: {len(content['images'])}")
        if content['video_url']:
            print(f"  è§†é¢‘: æœ‰")
        print(f"  å°é¢: {content['cover'][:60]}..." if content['cover'] else "  å°é¢: æ— ")
        
        print(f"\nğŸ‘¤ ä½œè€…ä¿¡æ¯:")
        author = detail['author']
        print(f"  æ˜µç§°: {author['nickname']}")
        print(f"  ç”¨æˆ·ID: {author['user_id']}")
        print(f"  ç­¾å: {author['desc'][:50]}{'...' if len(author['desc']) > 50 else ''}")
        
        if detail['hashtags']:
            print(f"\nğŸ·ï¸  è¯é¢˜æ ‡ç­¾:")
            for tag in detail['hashtags'][:5]:  # æœ€å¤šæ˜¾ç¤º5ä¸ª
                print(f"  #{tag['name']}")
    
    async def fetch_and_extract_user_notes(
        self,
        user_url: str,
        max_notes: Optional[int] = None,
        extract: bool = True,
        proxies: Optional[Dict] = None
    ) -> List[Dict[str, Any]]:
        """
        è·å–ç”¨æˆ·ç¬”è®°å¹¶æå–è¯¦ç»†ä¿¡æ¯
        
        Args:
            user_url: ç”¨æˆ·ä¸»é¡µURL
            max_notes: æœ€å¤šè·å–çš„ç¬”è®°æ•°
            extract: æ˜¯å¦æå–è¯¦ç»†ä¿¡æ¯ï¼ˆFalseåˆ™è¿”å›åŸå§‹æ•°æ®ï¼‰
            proxies: ä»£ç†è®¾ç½®
            
        Returns:
            ç¬”è®°åˆ—è¡¨ï¼ˆæå–åçš„è¯¦æƒ…æˆ–åŸå§‹æ•°æ®ï¼‰
        """
        # è·å–åŸå§‹ç¬”è®°æ•°æ®
        raw_notes = await self.fetch_all_user_notes(user_url, max_notes, proxies)
        
        if not extract:
            return raw_notes
        
        # æå–è¯¦ç»†ä¿¡æ¯
        print(f"\næ­£åœ¨æå– {len(raw_notes)} ä¸ªç¬”è®°çš„è¯¦ç»†ä¿¡æ¯...")
        details = self.extract_note_details_batch(raw_notes)
        print(f"âœ“ æå–å®Œæˆ")
        
        return details
    
    async def search_and_extract_notes(
        self,
        query: str,
        require_num: int,
        extract: bool = True,
        sort_type_choice: int = 0,
        note_type: int = 0,
        proxies: Optional[Dict] = None
    ) -> List[Dict[str, Any]]:
        """
        æœç´¢ç¬”è®°å¹¶æå–è¯¦ç»†ä¿¡æ¯
        
        Args:
            query: æœç´¢å…³é”®è¯
            require_num: æœç´¢æ•°é‡
            extract: æ˜¯å¦æå–è¯¦ç»†ä¿¡æ¯
            sort_type_choice: æ’åºæ–¹å¼
            note_type: ç¬”è®°ç±»å‹
            proxies: ä»£ç†è®¾ç½®
            
        Returns:
            ç¬”è®°åˆ—è¡¨ï¼ˆæå–åçš„è¯¦æƒ…æˆ–åŸå§‹æ•°æ®ï¼‰
        """
        # æœç´¢åŸå§‹ç¬”è®°æ•°æ®
        raw_notes = await self.search_notes(
            query=query,
            require_num=require_num,
            sort_type_choice=sort_type_choice,
            note_type=note_type,
            proxies=proxies
        )
        
        if not extract:
            return raw_notes
        
        # æå–è¯¦ç»†ä¿¡æ¯
        print(f"\næ­£åœ¨æå– {len(raw_notes)} ä¸ªç¬”è®°çš„è¯¦ç»†ä¿¡æ¯...")
        details = self.extract_note_details_batch(raw_notes)
        print(f"âœ“ æå–å®Œæˆ")
        
        return details
    
    def save_note_to_file(self, note: Dict[str, Any], user_id: Optional[str] = None) -> str:
        """
        ä¿å­˜å•ä¸ªç¬”è®°æ•°æ®åˆ°æ–‡ä»¶
        
        Args:
            note: ç¬”è®°æ•°æ®å­—å…¸
            user_id: ç”¨æˆ·IDï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›åˆ™ä¿å­˜åˆ°ç”¨æˆ·ç›®å½•ä¸‹ï¼‰
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        note_id = note.get('note_id', '')
        title = note.get('title', 'æ— æ ‡é¢˜')
        
        # æ ‡å‡†åŒ–æ–‡ä»¶å
        safe_title = norm_str(title)[:40]
        if not safe_title.strip():
            safe_title = 'æ— æ ‡é¢˜'
        
        # ç¡®å®šä¿å­˜è·¯å¾„
        if user_id:
            # å°è¯•å¤šç§æ–¹å¼è·å–æ˜µç§°ï¼ˆé€‚é…ä¸åŒçš„æ•°æ®ç»“æ„ï¼‰
            nickname = (note.get('nickname') or 
                       note.get('user', {}).get('nickname') or 
                       'unknown')
            safe_nickname = norm_str(nickname)[:20]
            if not safe_nickname.strip():
                safe_nickname = 'unknown'
            save_path = os.path.join(self.save_dir, f"{safe_nickname}_{user_id}")
        else:
            save_path = self.save_dir
        
        # åˆ›å»ºç›®å½•
        os.makedirs(save_path, exist_ok=True)
        
        # ç”Ÿæˆæ–‡ä»¶åï¼ˆåŒ…å«æ—¶é—´æˆ³ï¼‰
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"note_{note_id}_{timestamp}.json"
        filepath = os.path.join(save_path, filename)
        
        # ä¿å­˜JSONæ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(note, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ç¬”è®°å·²ä¿å­˜åˆ°: {filepath}")
        return filepath
    
    def save_notes_to_file(self, notes: List[Dict[str, Any]], user_id: Optional[str] = None) -> List[str]:
        """
        æ‰¹é‡ä¿å­˜ç¬”è®°æ•°æ®åˆ°æ–‡ä»¶
        
        Args:
            notes: ç¬”è®°æ•°æ®åˆ—è¡¨
            user_id: ç”¨æˆ·IDï¼ˆå¯é€‰ï¼‰
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        filepaths = []
        for note in notes:
            try:
                filepath = self.save_note_to_file(note, user_id)
                filepaths.append(filepath)
            except Exception as e:
                logger.error(f"ä¿å­˜ç¬”è®°å¤±è´¥: {e}")
        
        logger.info(f"æ‰¹é‡ä¿å­˜å®Œæˆï¼Œå…±ä¿å­˜ {len(filepaths)}/{len(notes)} ä¸ªç¬”è®°")
        return filepaths
    
    def save_user_notes_summary(self, user_id: str, notes: List[Dict[str, Any]]) -> str:
        """
        ä¿å­˜ç”¨æˆ·ç¬”è®°æ±‡æ€»æ–‡ä»¶
        
        Args:
            user_id: ç”¨æˆ·ID
            notes: ç”¨æˆ·çš„æ‰€æœ‰ç¬”è®°åˆ—è¡¨
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        if not notes:
            return ""
        
        # è·å–ç”¨æˆ·ä¿¡æ¯
        first_note = notes[0]
        # å°è¯•å¤šç§æ–¹å¼è·å–æ˜µç§°
        nickname = (first_note.get('nickname') or 
                   first_note.get('user', {}).get('nickname') or 
                   'unknown')
        safe_nickname = norm_str(nickname)[:20]
        if not safe_nickname.strip():
            safe_nickname = 'unknown'
        
        # åˆ›å»ºç”¨æˆ·ç›®å½•
        user_dir = os.path.join(self.save_dir, f"{safe_nickname}_{user_id}")
        os.makedirs(user_dir, exist_ok=True)
        
        # ç”Ÿæˆæ±‡æ€»æ•°æ®
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        summary = {
            'user_id': user_id,
            'nickname': nickname,
            'crawl_time': timestamp,
            'total_notes': len(notes),
            'notes': notes
        }
        
        # ä¿å­˜æ±‡æ€»æ–‡ä»¶
        filename = f"user_{user_id}_summary_{timestamp}.json"
        filepath = os.path.join(user_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ç”¨æˆ·ç¬”è®°æ±‡æ€»å·²ä¿å­˜åˆ°: {filepath}")
        return filepath
    
    async def search_and_save(
        self,
        query: str,
        require_num: int = 20,
        sort_type: int = 0,
        note_type: int = 0,
        save_format: str = "markdown",
        custom_save_dir: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        æœç´¢å…³é”®è¯å¹¶è‡ªåŠ¨ä¿å­˜ç¬”è®°åˆ°æ–‡ä»¶ - ç»Ÿä¸€æ¥å£
        
        æŒ‰å…³é”®è¯åˆ†ç±»ä¿å­˜ï¼Œæ¯ä¸ªå…³é”®è¯ä¸€ä¸ªæ–‡ä»¶å¤¹
        
        Args:
            query: æœç´¢å…³é”®è¯
            require_num: æœç´¢æ•°é‡ï¼Œé»˜è®¤20
            sort_type: æ’åºæ–¹å¼ (0=ç»¼åˆ, 1=æœ€æ–°, 2=ç‚¹èµ, 3=è¯„è®º, 4=æ”¶è—)
            note_type: ç¬”è®°ç±»å‹ (0=ä¸é™, 1=è§†é¢‘, 2=å›¾æ–‡)
            save_format: ä¿å­˜æ ¼å¼ "markdown" æˆ– "json" æˆ– "both"
            custom_save_dir: è‡ªå®šä¹‰ä¿å­˜æ ¹ç›®å½•ï¼Œå¦‚æœæä¾›åˆ™è¦†ç›–é»˜è®¤çš„ save_dir
            
        Returns:
            åŒ…å«æœç´¢ç»“æœå’Œä¿å­˜ä¿¡æ¯çš„å­—å…¸:
            {
                'success': bool,           # æ˜¯å¦æˆåŠŸ
                'query': str,              # æœç´¢å…³é”®è¯
                'total_found': int,        # æ‰¾åˆ°çš„ç¬”è®°æ•°
                'saved': int,              # æˆåŠŸä¿å­˜çš„ç¬”è®°æ•°
                'failed': int,             # å¤±è´¥çš„ç¬”è®°æ•°
                'save_directory': str,     # ä¿å­˜çš„æ ¹ç›®å½•
                'directories': List[str],  # æ‰€æœ‰ç¬”è®°ç›®å½•
                'statistics': Dict         # ç»Ÿè®¡æ•°æ®ï¼ˆç‚¹èµã€æ”¶è—ç­‰ï¼‰
            }
        
        Example:
            >>> provider = XiaohongshuProvider()
            >>> # ä½¿ç”¨é»˜è®¤ç›®å½•
            >>> result = await provider.search_and_save("Pythonç¼–ç¨‹", require_num=10)
            >>> 
            >>> # è‡ªå®šä¹‰ä¿å­˜ç›®å½•
            >>> result = await provider.search_and_save(
            ...     query="ç¾é£Ÿ",
            ...     require_num=5,
            ...     custom_save_dir="D:/my_notes/xiaohongshu"
            ... )
            >>> print(f"æˆåŠŸä¿å­˜ {result['saved']} ä¸ªç¬”è®°")
            >>> print(f"ä¿å­˜ä½ç½®: {result['save_directory']}")
        """
        start_time = datetime.now()
        
        logger.info(f"å¼€å§‹æœç´¢å¹¶ä¿å­˜: å…³é”®è¯='{query}', æ•°é‡={require_num}")
        
        try:
            # 1. æœç´¢ç¬”è®°ï¼ˆå·²åŒ…å«æ‰¹é‡è·å–è¯¦æƒ…ï¼‰
            print(f"\n{'='*70}")
            print(f"ğŸ” æœç´¢å…³é”®è¯: {query}")
            print(f"ğŸ“Š æœç´¢æ•°é‡: {require_num}")
            print(f"{'='*70}\n")
            
            raw_notes = await self.search_notes(
                query=query,
                require_num=require_num,
                sort_type_choice=sort_type,
                note_type=note_type
            )
            
            if not raw_notes:
                logger.warning(f"æœªæœç´¢åˆ°ä»»ä½•ç¬”è®°: {query}")
                return {
                    'success': False,
                    'query': query,
                    'total_found': 0,
                    'saved': 0,
                    'failed': 0,
                    'save_directory': '',
                    'directories': [],
                    'statistics': {
                        'error': 'æœªæœç´¢åˆ°ä»»ä½•ç¬”è®°',
                        'duration_seconds': (datetime.now() - start_time).total_seconds()
                    }
                }
            
            # 2. æå–è¯¦ç»†ä¿¡æ¯
            print(f"\nğŸ“ æ­£åœ¨æå– {len(raw_notes)} ä¸ªç¬”è®°çš„è¯¦ç»†ä¿¡æ¯...")
            details = self.extract_note_details_batch(raw_notes)
            print(f"âœ… è¯¦æƒ…æå–å®Œæˆ\n")
            
            # 3. æŒ‰å…³é”®è¯åˆ›å»ºä¿å­˜ç›®å½•
            import re
            # ä½¿ç”¨è‡ªå®šä¹‰ç›®å½•æˆ–é»˜è®¤ç›®å½•
            base_save_dir = custom_save_dir if custom_save_dir else self.save_dir
            
            # æ¸…ç†å…³é”®è¯ä½œä¸ºæ–‡ä»¶å¤¹å
            safe_query = re.sub(r'[\\/:*?"<>|]', '_', query).strip()[:30]
            if not safe_query:
                safe_query = 'search_results'
            keyword_dir = os.path.join(base_save_dir, safe_query)
            os.makedirs(keyword_dir, exist_ok=True)
            
            # 4. ä¿å­˜ç¬”è®°
            saved_directories = []
            successful_saves = 0
            
            print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜ç¬”è®°åˆ°: {keyword_dir}")
            
            for i, (raw_note, detail) in enumerate(zip(raw_notes, details), 1):
                try:
                    # è·å–ç¬”è®°ä¿¡æ¯
                    title = detail.get('title', 'æ— æ ‡é¢˜')
                    note_id = detail.get('note_id', '')
                    author = detail.get('author', {}).get('nickname', 'æœªçŸ¥ä½œè€…')
                    note_url = detail.get('link', '')
                    
                    # ç”Ÿæˆå”¯ä¸€IDå’Œç›®å½•å
                    import hashlib
                    article_id = hashlib.md5(f"{note_url}_{title}".encode()).hexdigest()[:12]
                    # æ¸…ç†æ ‡é¢˜ä½œä¸ºæ–‡ä»¶å¤¹å
                    safe_title = re.sub(r'[\\/:*?"<>|]', '_', title).strip()[:100]
                    if not safe_title:
                        safe_title = 'untitled'
                    article_dir_name = f"{article_id}_{safe_title}"
                    article_dir = os.path.join(keyword_dir, article_dir_name)
                    
                    # åˆ›å»ºç›®å½•ç»“æ„
                    os.makedirs(article_dir, exist_ok=True)
                    images_dir = os.path.join(article_dir, "images")
                    os.makedirs(images_dir, exist_ok=True)
                    
                    # æ„å»ºå†…å®¹ï¼ˆä½¿ç”¨ raw_note ä»¥è·å–æ­£ç¡®çš„ç»Ÿè®¡æ•°æ®ï¼‰
                    content_text = self._build_note_content_text(raw_note, detail)
                    markdown_content = self._build_note_content_markdown(raw_note, detail)
                    
                    # ä¿å­˜æ–‡æœ¬å†…å®¹
                    text_file = os.path.join(article_dir, f"{safe_title}.txt")
                    with open(text_file, 'w', encoding='utf-8') as f:
                        f.write(content_text)
                    
                    # ä¿å­˜ Markdown å†…å®¹
                    if save_format in ["markdown", "both"]:
                        markdown_file = os.path.join(article_dir, f"{safe_title}.md")
                        with open(markdown_file, 'w', encoding='utf-8') as f:
                            f.write(markdown_content)
                    
                    # ä¿å­˜åŸå§‹ JSON æ•°æ®
                    if save_format in ["json", "both"]:
                        raw_data_path = os.path.join(article_dir, "raw_data.json")
                        with open(raw_data_path, 'w', encoding='utf-8') as f:
                            json.dump(raw_note, f, ensure_ascii=False, indent=2)
                    
                    # ä¸‹è½½å›¾ç‰‡
                    storage_info = {'article_dir': article_dir}
                    await self._download_note_images(raw_note, storage_info)
                    
                    # åˆ›å»ºå¢å¼ºçš„å…ƒæ•°æ®ï¼ˆåŒ…å«ç»Ÿè®¡æ•°æ®ï¼‰
                    statistics = detail.get('statistics', {})
                    metadata = {
                        "article_id": article_id,
                        "title": title,
                        "author": author,
                        "url": note_url,
                        "platform": self.platform_name,
                        "keyword": query,
                        "note_id": note_id,
                        "note_type": raw_note.get('note_type', ''),
                        "created_at": datetime.now().isoformat(),
                        "upload_time": raw_note.get('upload_time', ''),
                        "ip_location": raw_note.get('ip_location', ''),
                        "statistics": {
                            "liked_count": int(raw_note.get('liked_count', 0)) if isinstance(raw_note.get('liked_count'), (int, str)) else 0,
                            "collected_count": int(raw_note.get('collected_count', 0)) if isinstance(raw_note.get('collected_count'), (int, str)) else 0,
                            "comment_count": int(raw_note.get('comment_count', 0)) if isinstance(raw_note.get('comment_count'), (int, str)) else 0,
                            "share_count": int(raw_note.get('share_count', 0)) if isinstance(raw_note.get('share_count'), (int, str)) else 0,
                            "images_count": len(raw_note.get('image_list', [])),
                            "content_length": len(content_text)
                        },
                        "tags": raw_note.get('tags', []),
                        "files": {
                            "text_file": f"{safe_title}.txt",
                            "markdown_file": f"{safe_title}.md" if save_format in ["markdown", "both"] else None,
                            "raw_data": "raw_data.json" if save_format in ["json", "both"] else None
                        }
                    }
                    
                    # ä¿å­˜å…ƒæ•°æ®
                    metadata_file = os.path.join(article_dir, "metadata.json")
                    with open(metadata_file, 'w', encoding='utf-8') as f:
                        json.dump(metadata, f, ensure_ascii=False, indent=2)
                    
                    saved_directories.append(article_dir)
                    successful_saves += 1
                    
                    if (i % 5 == 0) or (i == len(raw_notes)):
                        print(f"   å·²ä¿å­˜: {i}/{len(raw_notes)} ä¸ªç¬”è®°")
                    
                except Exception as e:
                    logger.error(f"ä¿å­˜ç¬”è®°å¤±è´¥ {detail.get('title', 'Unknown')}: {e}")
                    import traceback

                    traceback.print_exc()
                    continue
            
            print(f"âœ… ä¿å­˜å®Œæˆ: {successful_saves}/{len(raw_notes)} ä¸ªç¬”è®°\n")
            
            # 5. ç»Ÿè®¡æ•°æ®
            total_likes = sum(int(raw_note.get('liked_count', 0)) if isinstance(raw_note.get('liked_count'), (int, str)) else 0 for raw_note in raw_notes)
            total_collects = sum(int(raw_note.get('collected_count', 0)) if isinstance(raw_note.get('collected_count'), (int, str)) else 0 for raw_note in raw_notes)
            total_comments = sum(int(raw_note.get('comment_count', 0)) if isinstance(raw_note.get('comment_count'), (int, str)) else 0 for raw_note in raw_notes)
            total_shares = sum(int(raw_note.get('share_count', 0)) if isinstance(raw_note.get('share_count'), (int, str)) else 0 for raw_note in raw_notes)
            
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            # æ‰“å°ç»“æœæ‘˜è¦
            print(f"\n{'='*70}")
            print(f"âœ… æœç´¢å¹¶ä¿å­˜å®Œæˆ!")
            print(f"{'='*70}")
            print(f"ğŸ” æœç´¢å…³é”®è¯: {query}")
            print(f"ğŸ“Š æ‰¾åˆ°ç¬”è®°: {len(raw_notes)} ä¸ª")
            print(f"ğŸ’¾ æˆåŠŸä¿å­˜: {successful_saves} ä¸ª")
            print(f"ğŸ“ ä¿å­˜ä½ç½®: {keyword_dir}")
            print(f"â±ï¸  è€—æ—¶: {duration:.1f} ç§’")
            print(f"\nğŸ“ˆ äº’åŠ¨æ•°æ®ç»Ÿè®¡:")
            print(f"   æ€»ç‚¹èµ: {total_likes:,}")
            print(f"   æ€»æ”¶è—: {total_collects:,}")
            print(f"   æ€»è¯„è®º: {total_comments:,}")
            print(f"   æ€»åˆ†äº«: {total_shares:,}")
            print(f"{'='*70}\n")
            
            result = {
                'success': True,
                'query': query,
                'total_found': len(raw_notes),
                'saved': successful_saves,
                'failed': len(raw_notes) - successful_saves,
                'save_directory': keyword_dir,
                'directories': saved_directories,
                'statistics': {
                    'total_likes': total_likes,
                    'total_collects': total_collects,
                    'total_comments': total_comments,
                    'total_shares': total_shares,
                    'avg_likes': total_likes / len(raw_notes) if raw_notes else 0,
                    'avg_collects': total_collects / len(raw_notes) if raw_notes else 0,
                    'duration_seconds': duration
                }
            }
            
            logger.info(f"æœç´¢ä¿å­˜å®Œæˆ: {query}, æˆåŠŸ: {successful_saves}/{len(raw_notes)}")
            return result
            
        except Exception as e:
            logger.error(f"æœç´¢å¹¶ä¿å­˜å¤±è´¥: {e}", exc_info=True)
            return {
                'success': False,
                'query': query,
                'total_found': 0,
                'saved': 0,
                'failed': 0,
                'save_directory': '',
                'directories': [],
                'statistics': {
                    'error': str(e),
                    'duration_seconds': (datetime.now() - start_time).total_seconds()
                }
            }
    
    def _build_note_content_text(self, raw_note: Dict[str, Any], detail: Dict[str, Any]) -> str:
        """
        æ„å»ºç¬”è®°çš„çº¯æ–‡æœ¬å†…å®¹
        
        Args:
            raw_note: åŸå§‹ç¬”è®°æ•°æ®ï¼ˆåŒ…å«æ­£ç¡®çš„ç»Ÿè®¡æ•°æ®ï¼‰
            detail: ç¬”è®°è¯¦æƒ…å­—å…¸
            
        Returns:
            çº¯æ–‡æœ¬å†…å®¹
        """
        parts = []
        
        # æ ‡é¢˜
        title = raw_note.get('title', detail.get('title', ''))
        if title:
            parts.append(title)
            parts.append('')
        
        # æè¿°
        desc = raw_note.get('desc', detail.get('desc', ''))
        if desc:
            parts.append(desc)
            parts.append('')
        
        # äº’åŠ¨æ•°æ®ï¼ˆä½¿ç”¨ raw_note çš„æ•°æ®ï¼‰
        liked_count = raw_note.get('liked_count', 0)
        collected_count = raw_note.get('collected_count', 0)
        comment_count = raw_note.get('comment_count', 0)
        share_count = raw_note.get('share_count', 0)
        
        # è½¬æ¢ä¸ºæ•´æ•°ï¼ˆæœ‰äº›å¯èƒ½æ˜¯å­—ç¬¦ä¸²ï¼‰
        # æ ¼å¼åŒ–æ•°å­—æ˜¾ç¤ºï¼ˆå¤„ç†ä¸­æ–‡å•ä½å¦‚"1.4ä¸‡"ï¼‰
        def format_count(count):
            """æ ¼å¼åŒ–æ•°å­—,ä¿æŒåŸæ ·æˆ–è½¬æ¢ä¸ºå¸¦åƒä½åˆ†éš”ç¬¦çš„æ ¼å¼"""
            if isinstance(count, str):
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²(å¦‚"1.4ä¸‡"),ç›´æ¥è¿”å›
                return count
            try:
                # å¦‚æœæ˜¯æ•°å­—,æ·»åŠ åƒä½åˆ†éš”ç¬¦
                return f"{int(count):,}"
            except:
                return str(count) if count else "0"
        
        parts.append(f"ç‚¹èµ: {format_count(liked_count)}")
        parts.append(f"æ”¶è—: {format_count(collected_count)}")
        parts.append(f"è¯„è®º: {format_count(comment_count)}")
        parts.append(f"åˆ†äº«: {format_count(share_count)}")
        parts.append('')
        
        # é“¾æ¥
        link = raw_note.get('note_url', detail.get('link', ''))
        if link:
            # ç§»é™¤ xsec_token å‚æ•°
            if '?' in link:
                link = link.split('?')[0]
            parts.append(f"åŸæ–‡é“¾æ¥: {link}")
        
        return '\n'.join(parts)
    
    def _build_note_content_markdown(self, raw_note: Dict[str, Any], detail: Dict[str, Any]) -> str:
        """
        æ„å»ºç¬”è®°çš„ Markdown å†…å®¹ï¼ˆå’Œå…¶ä»–å¹³å°æ ¼å¼ä¸€è‡´ï¼‰
        
        Args:
            raw_note: åŸå§‹ç¬”è®°æ•°æ®ï¼ˆåŒ…å«æ­£ç¡®çš„ç»Ÿè®¡æ•°æ®ï¼‰
            detail: ç¬”è®°è¯¦æƒ…å­—å…¸
            
        Returns:
            Markdown æ ¼å¼çš„æ–‡æœ¬
        """
        # åŸºæœ¬ä¿¡æ¯
        title = raw_note.get('title', detail.get('title', 'æ— æ ‡é¢˜'))
        desc = raw_note.get('desc', detail.get('desc', ''))
        note_type = raw_note.get('note_type', '')
        author_name = raw_note.get('nickname', '')
        upload_time = raw_note.get('upload_time', '')
        ip_location = raw_note.get('ip_location', '')
        
        # é“¾æ¥ï¼ˆå»é™¤ tokenï¼‰
        link = raw_note.get('note_url', detail.get('link', ''))
        if '?' in link:
            link = link.split('?')[0]
        
        # äº’åŠ¨æ•°æ®ï¼ˆä½¿ç”¨ raw_note çš„æ•°æ®ï¼‰
        liked_count = raw_note.get('liked_count', 0)
        collected_count = raw_note.get('collected_count', 0)
        comment_count = raw_note.get('comment_count', 0)
        share_count = raw_note.get('share_count', 0)
        
        # æ ¼å¼åŒ–æ•°å­—æ˜¾ç¤ºï¼ˆå¤„ç†ä¸­æ–‡å•ä½å¦‚"1.4ä¸‡"ï¼‰
        def format_count(count):
            """æ ¼å¼åŒ–æ•°å­—,ä¿æŒåŸæ ·æˆ–è½¬æ¢ä¸ºå¸¦åƒä½åˆ†éš”ç¬¦çš„æ ¼å¼"""
            if isinstance(count, str):
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²(å¦‚"1.4ä¸‡"),ç›´æ¥è¿”å›
                return count
            try:
                # å¦‚æœæ˜¯æ•°å­—,æ·»åŠ åƒä½åˆ†éš”ç¬¦
                return f"{int(count):,}"
            except:
                return str(count) if count else "0"
        
        # æ ‡ç­¾
        tags = raw_note.get('tags', [])
        tags_str = ' '.join([f"#{tag}" for tag in tags]) if tags else 'æ— æ ‡ç­¾'
        
        # æ„å»ºMarkdownå†…å®¹
        md_content = f"""# {title}

---

## ğŸ“ åŸºæœ¬ä¿¡æ¯

- **ä½œè€…**: {author_name}
- **å‘å¸ƒæ—¶é—´**: {upload_time}
- **ç¬”è®°ç±»å‹**: {note_type}
- **IPå½’å±åœ°**: {ip_location}
- **åŸæ–‡é“¾æ¥**: [{link}]({link})

## ğŸ“Š äº’åŠ¨æ•°æ®

- ğŸ‘ ç‚¹èµ: **{format_count(liked_count)}**
- â­ æ”¶è—: **{format_count(collected_count)}**
- ğŸ’¬ è¯„è®º: **{format_count(comment_count)}**
- ğŸ”— åˆ†äº«: **{format_count(share_count)}**

## ğŸ“„ å†…å®¹æè¿°

{desc}

## ğŸ·ï¸ è¯é¢˜æ ‡ç­¾

{tags_str}

---

*æœ¬æ–‡ä»¶ç”±å°çº¢ä¹¦çˆ¬è™«è‡ªåŠ¨ç”Ÿæˆ*  
*ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        return md_content

    async def _download_note_images(self, raw_note: Dict[str, Any], storage_info: Dict[str, Any]) -> None:
        """
        ä¸‹è½½ç¬”è®°çš„æ‰€æœ‰å›¾ç‰‡
        
        Args:
            raw_note: åŸå§‹ç¬”è®°æ•°æ®
            storage_info: å­˜å‚¨ä¿¡æ¯å­—å…¸
        """
        try:
            # è·å–å›¾ç‰‡åˆ—è¡¨
            image_list = raw_note.get('image_list', [])
            if not image_list:
                return
            
            # åˆ›å»º images ç›®å½•
            images_dir = os.path.join(storage_info['article_dir'], 'images')
            os.makedirs(images_dir, exist_ok=True)
            
            # ä¸‹è½½æ‰€æœ‰å›¾ç‰‡
            downloaded_count = 0
            total_images = len(image_list)
            
            timeout = httpx.Timeout(timeout=30)
            async with httpx.Client(timeout=timeout) as session:
                for idx, image_url in enumerate(image_list, 1):
                    try:
                        if not image_url:
                            continue
                        
                        # ä¸‹è½½å›¾ç‰‡
                        async with session.get(image_url) as response:
                            if response.status == 200:
                                content = await response.read()
                                
                                # æ™ºèƒ½æ£€æµ‹å›¾ç‰‡æ ¼å¼
                                content_type = response.headers.get('Content-Type')
                                ext = get_file_extension(content_type=content_type, url=image_url, content=content)
                                
                                # ç”Ÿæˆæ–‡ä»¶åï¼šä½¿ç”¨åºå·å‘½å
                                filename = f"image_{idx:03d}.{ext}"
                                filepath = os.path.join(images_dir, filename)
                                
                                with open(filepath, 'wb') as f:
                                    f.write(content)
                                downloaded_count += 1
                            else:
                                logger.warning(f"å›¾ç‰‡ä¸‹è½½å¤±è´¥ (çŠ¶æ€ç  {response.status}): {image_url}")
                        
                        # é¿å…è¯·æ±‚è¿‡å¿«
                        await asyncio.sleep(0.2)
                        
                    except Exception as e:
                        logger.warning(f"ä¸‹è½½å›¾ç‰‡ {idx}/{total_images} å¤±è´¥: {e}")
                        continue
            
            if downloaded_count > 0:
                logger.info(f"   ğŸ“· æˆåŠŸä¸‹è½½ {downloaded_count}/{total_images} å¼ å›¾ç‰‡")
            
        except Exception as e:
            logger.error(f"ä¸‹è½½ç¬”è®°å›¾ç‰‡å¤±è´¥: {e}")

    async def close(self):
        """å…³é—­è¿æ¥ï¼ˆä¸ºäº†æ¥å£ä¸€è‡´æ€§ï¼‰"""
        # å°çº¢ä¹¦çˆ¬è™«æš‚æ— éœ€ç‰¹æ®Šå…³é—­æ“ä½œï¼Œä¿ç•™æ¥å£ä»¥ä¿æŒä¸€è‡´æ€§
        pass


# ============================================================================
# å…¼å®¹æ€§å°è£…ï¼ˆä¿æŒæ—§æ¥å£å¯ç”¨ï¼‰
# ============================================================================


class Data_Spider:
    """
    å…¼å®¹æ€§ç±»ï¼Œä¿æŒæ—§æ¥å£å¯ç”¨ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰
    
    âš ï¸ å·²åºŸå¼ƒ (Deprecated)ï¼šå»ºè®®ä½¿ç”¨å¼‚æ­¥ç‰ˆæœ¬ XiaohongshuProvider
    æ­¤ç±»ä»…ä¸ºå‘åå…¼å®¹è€Œä¿ç•™ï¼ŒåŠŸèƒ½è¾ƒå°‘ä¸”ç¼ºå°‘å»¶è¿Ÿæ§åˆ¶
    å°†åœ¨æœªæ¥ç‰ˆæœ¬ä¸­ç§»é™¤
    
    æ¨èè¿ç§»æŒ‡å—: å‚è€ƒ docs/XHS_ASYNC_MIGRATION.md
    """

    def __init__(self, save_dir: str = "data/xiaohongshu"):
        """
        åˆå§‹åŒ–æ•°æ®çˆ¬è™«
        
        Args:
            save_dir: æ•°æ®ä¿å­˜ç›®å½•ï¼Œé»˜è®¤"data/xiaohongshu"
        """
        self.xhs_apis = XHS_Apis()
        self.save_dir = save_dir
        
        # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
        os.makedirs(save_dir, exist_ok=True)

    def spider_note(self, note_url: str, cookies_str: str, proxies=None, max_retries=3, base_delay=5):
        """
        çˆ¬å–ä¸€ä¸ªç¬”è®°çš„ä¿¡æ¯ï¼Œå¸¦é‡è¯•å’Œå»¶æ—¶æœºåˆ¶
        :param note_url: ç¬”è®°URL
        :param cookies_str: cookieså­—ç¬¦ä¸²
        :param proxies: ä»£ç†è®¾ç½®
        :param max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        :param base_delay: åŸºç¡€å»¶æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        :return: (success, msg, note_info)
        """
        note_info = None
        success = False
        msg = "æœªå¼€å§‹å¤„ç†"
        
        for attempt in range(max_retries + 1):
            try:
                if attempt > 0:
                    # è®¡ç®—å»¶æ—¶æ—¶é—´ï¼Œæ¯æ¬¡é‡è¯•å¢åŠ å»¶æ—¶
                    delay = base_delay * (2 ** (attempt - 1)) + random.uniform(1, 3)
                    logger.info(f'ç¬¬{attempt}æ¬¡é‡è¯•ï¼Œç­‰å¾…{delay:.1f}ç§’...')
                    time.sleep(delay)
                
                success, msg, response_data = self.xhs_apis.get_note_info(note_url, cookies_str, proxies or {})
                logger.info(f'APIè°ƒç”¨ç»“æœ (å°è¯•{attempt + 1}): success={success}, msg={msg}')
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯é™æµé”™è¯¯
                if response_data and isinstance(response_data, dict):
                    if response_data.get('code') == 300013:  # è®¿é—®é¢‘æ¬¡å¼‚å¸¸
                        logger.warning(f'æ£€æµ‹åˆ°è®¿é—®é¢‘æ¬¡é™åˆ¶ï¼Œç¬¬{attempt + 1}æ¬¡å°è¯•')
                        if attempt < max_retries:
                            continue
                        else:
                            success = False
                            msg = 'é‡è¯•æ¬¡æ•°ç”¨å°½ï¼Œä»ç„¶é‡åˆ°è®¿é—®é¢‘æ¬¡é™åˆ¶'
                            break
                
                if success and response_data:
                    # æ£€æŸ¥æ•°æ®ç»“æ„
                    if not isinstance(response_data, dict):
                        success = False
                        msg = f'APIè¿”å›æ•°æ®ä¸æ˜¯å­—å…¸æ ¼å¼: {type(response_data)}'
                    elif 'data' not in response_data:
                        success = False
                        msg = f'APIè¿”å›æ•°æ®ä¸­æ²¡æœ‰dataå­—æ®µï¼Œå¯ç”¨å­—æ®µ: {list(response_data.keys())}'
                    elif not isinstance(response_data['data'], dict):
                        success = False
                        msg = f'dataå­—æ®µä¸æ˜¯å­—å…¸æ ¼å¼: {type(response_data["data"])}'
                    elif 'items' not in response_data['data']:
                        success = False
                        msg = f'dataä¸­æ²¡æœ‰itemså­—æ®µï¼Œå¯ç”¨å­—æ®µ: {list(response_data["data"].keys())}'
                    elif not isinstance(response_data['data']['items'], list):
                        success = False
                        msg = f'itemsä¸æ˜¯åˆ—è¡¨æ ¼å¼: {type(response_data["data"]["items"])}'
                    elif len(response_data['data']['items']) == 0:
                        success = False
                        msg = 'itemsåˆ—è¡¨ä¸ºç©º'
                    else:
                        note_info = response_data['data']['items'][0]
                        note_info['url'] = note_url
                        note_info = handle_note_info(note_info)
                        logger.info('ç¬”è®°ä¿¡æ¯å¤„ç†æˆåŠŸ')
                        break  # æˆåŠŸå¤„ç†ï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                else:
                    logger.warning(f'APIè°ƒç”¨å¤±è´¥æˆ–è¿”å›æ•°æ®ä¸ºç©º: success={success}')
                    if attempt < max_retries:
                        continue
                    
                break  # å¦‚æœä¸éœ€è¦é‡è¯•ï¼Œè·³å‡ºå¾ªç¯
                    
            except Exception as e:
                success = False
                msg = f'å¤„ç†ç¬”è®°æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}'
                logger.error(f'çˆ¬å–ç¬”è®°å¼‚å¸¸ (å°è¯•{attempt + 1}): {e}', exc_info=True)
                if attempt < max_retries:
                    continue
                break
            
        logger.info(f'çˆ¬å–ç¬”è®°ä¿¡æ¯å®Œæˆ {note_url}: success={success}, msg={msg}')
        return success, msg, note_info

    def spider_some_note(self, notes: list, cookies_str: str, proxies=None):
        """
        çˆ¬å–ä¸€äº›ç¬”è®°çš„ä¿¡æ¯
        :param notes: ç¬”è®°URLåˆ—è¡¨
        :param cookies_str: cookieså­—ç¬¦ä¸²
        :param proxies: ä»£ç†è®¾ç½®
        :return: ç¬”è®°ä¿¡æ¯åˆ—è¡¨
        """
        note_list = []
        for note_url in notes:
            success, msg, note_info = self.spider_note(note_url, cookies_str, proxies)
            if note_info is not None and success:
                note_list.append(note_info)
        return note_list

    def spider_user_all_note(self, user_url: str, cookies_str: str, proxies=None):
        """
        çˆ¬å–ä¸€ä¸ªç”¨æˆ·çš„æ‰€æœ‰ç¬”è®°
        :param user_url: ç”¨æˆ·ä¸»é¡µURL
        :param cookies_str: cookieså­—ç¬¦ä¸²
        :param proxies: ä»£ç†è®¾ç½®
        :return: (note_data, success, msg)
        """
        note_list = []
        try:
            success, msg, all_note_info = self.xhs_apis.get_user_all_notes(user_url, cookies_str, proxies or {})
            if success:
                logger.info(f'ç”¨æˆ· {user_url} ä½œå“æ•°é‡: {len(all_note_info)}')
                for simple_note_info in all_note_info:
                    note_url = f"https://www.xiaohongshu.com/explore/{simple_note_info['note_id']}?xsec_token={simple_note_info['xsec_token']}"
                    note_list.append(note_url)
                note_data = self.spider_some_note(note_list, cookies_str, proxies)
                return note_data, success, msg
        except Exception as e:
            success = False
            msg = str(e)
        logger.info(f'çˆ¬å–ç”¨æˆ·æ‰€æœ‰è§†é¢‘ {user_url}: {success}, msg: {msg}')
        return [], success, msg

    def spider_some_search_note(self, query: str, require_num: int, cookies_str: str, sort_type_choice=0, note_type=0, note_time=0, note_range=0, pos_distance=0, geo: Optional[dict] = None, proxies: Optional[dict] = None):
        """
        æŒ‡å®šæ•°é‡æœç´¢ç¬”è®°ï¼Œè®¾ç½®æ’åºæ–¹å¼å’Œç¬”è®°ç±»å‹å’Œç¬”è®°æ•°é‡
        :param query: æœç´¢çš„å…³é”®è¯
        :param require_num: æœç´¢çš„æ•°é‡
        :param cookies_str: cookieså­—ç¬¦ä¸²
        :param sort_type_choice: æ’åºæ–¹å¼ 0 ç»¼åˆæ’åº, 1 æœ€æ–°, 2 æœ€å¤šç‚¹èµ, 3 æœ€å¤šè¯„è®º, 4 æœ€å¤šæ”¶è—
        :param note_type: ç¬”è®°ç±»å‹ 0 ä¸é™, 1 è§†é¢‘ç¬”è®°, 2 æ™®é€šç¬”è®°
        :param note_time: ç¬”è®°æ—¶é—´ 0 ä¸é™, 1 ä¸€å¤©å†…, 2 ä¸€å‘¨å†…å¤©, 3 åŠå¹´å†…
        :param note_range: ç¬”è®°èŒƒå›´ 0 ä¸é™, 1 å·²çœ‹è¿‡, 2 æœªçœ‹è¿‡, 3 å·²å…³æ³¨
        :param pos_distance: ä½ç½®è·ç¦» 0 ä¸é™, 1 åŒåŸ, 2 é™„è¿‘ æŒ‡å®šè¿™ä¸ªå¿…é¡»è¦æŒ‡å®š geo
        :param geo: åœ°ç†ä½ç½®ä¿¡æ¯å­—å…¸ï¼Œä¾‹å¦‚ {"latitude": 39.9725, "longitude": 116.4207}
        :param proxies: ä»£ç†è®¾ç½®
        :return: (note_data, success, msg)
        """
        note_list = []
        try:
            success, msg, notes = self.xhs_apis.search_some_note(
                query, require_num, cookies_str, sort_type_choice, 
                note_type, note_time, note_range, pos_distance, 
                str(geo) if geo else "", proxies or {}
            )
            if success:
                notes = list(filter(lambda x: x['model_type'] == "note", notes))
                logger.info(f'æœç´¢å…³é”®è¯ {query} ç¬”è®°æ•°é‡: {len(notes)}')
                for note in notes:
                    note_url = f"https://www.xiaohongshu.com/explore/{note['id']}?xsec_token={note['xsec_token']}"
                    note_list.append(note_url)
                note_data = self.spider_some_note(note_list, cookies_str, proxies)
                return note_data, success, msg
        except Exception as e:
            success = False
            msg = str(e)
        logger.info(f'æœç´¢å…³é”®è¯ {query} ç¬”è®°: {success}, msg: {msg}')
        return [], success, msg
    
    def save_note_to_file(self, note: Dict[str, Any], user_id: Optional[str] = None) -> str:
        """
        ä¿å­˜å•ä¸ªç¬”è®°æ•°æ®åˆ°æ–‡ä»¶
        
        Args:
            note: ç¬”è®°æ•°æ®å­—å…¸
            user_id: ç”¨æˆ·IDï¼ˆå¯é€‰ï¼‰
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        note_id = note.get('note_id', '')
        title = note.get('title', 'æ— æ ‡é¢˜')
        
        # æ ‡å‡†åŒ–æ–‡ä»¶å
        safe_title = norm_str(title)[:40]
        if not safe_title.strip():
            safe_title = 'æ— æ ‡é¢˜'
        
        # ç¡®å®šä¿å­˜è·¯å¾„
        if user_id:
            # å°è¯•å¤šç§æ–¹å¼è·å–æ˜µç§°ï¼ˆé€‚é…ä¸åŒçš„æ•°æ®ç»“æ„ï¼‰
            nickname = (note.get('nickname') or 
                       note.get('user', {}).get('nickname') or 
                       'unknown')
            safe_nickname = norm_str(nickname)[:20]
            if not safe_nickname.strip():
                safe_nickname = 'unknown'
            save_path = os.path.join(self.save_dir, f"{safe_nickname}_{user_id}")
        else:
            save_path = self.save_dir
        
        # åˆ›å»ºç›®å½•
        os.makedirs(save_path, exist_ok=True)
        
        # ç”Ÿæˆæ–‡ä»¶åï¼ˆåŒ…å«æ—¶é—´æˆ³ï¼‰
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"note_{note_id}_{timestamp}.json"
        filepath = os.path.join(save_path, filename)
        
        # ä¿å­˜JSONæ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(note, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ç¬”è®°å·²ä¿å­˜åˆ°: {filepath}")
        return filepath
    
    def save_notes_to_file(self, notes: List[Dict[str, Any]], user_id: Optional[str] = None, save_summary: bool = True) -> List[str]:
        """
        æ‰¹é‡ä¿å­˜ç¬”è®°æ•°æ®åˆ°æ–‡ä»¶
        
        Args:
            notes: ç¬”è®°æ•°æ®åˆ—è¡¨
            user_id: ç”¨æˆ·IDï¼ˆå¯é€‰ï¼‰
            save_summary: æ˜¯å¦ä¿å­˜æ±‡æ€»æ–‡ä»¶ï¼Œé»˜è®¤True
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        if not notes:
            logger.warning("æ²¡æœ‰ç¬”è®°éœ€è¦ä¿å­˜")
            return []
        
        filepaths = []
        
        # ä¿å­˜å•ä¸ªç¬”è®°æ–‡ä»¶
        for note in notes:
            try:
                filepath = self.save_note_to_file(note, user_id)
                filepaths.append(filepath)
            except Exception as e:
                logger.error(f"ä¿å­˜ç¬”è®°å¤±è´¥: {e}")
        
        # ä¿å­˜æ±‡æ€»æ–‡ä»¶
        if save_summary and user_id:
            try:
                self.save_user_notes_summary(user_id, notes)
            except Exception as e:
                logger.error(f"ä¿å­˜æ±‡æ€»æ–‡ä»¶å¤±è´¥: {e}")
        
        logger.info(f"æ‰¹é‡ä¿å­˜å®Œæˆï¼Œå…±ä¿å­˜ {len(filepaths)}/{len(notes)} ä¸ªç¬”è®°")
        return filepaths
    
    def save_user_notes_summary(self, user_id: str, notes: List[Dict[str, Any]]) -> str:
        """
        ä¿å­˜ç”¨æˆ·ç¬”è®°æ±‡æ€»æ–‡ä»¶
        
        Args:
            user_id: ç”¨æˆ·ID
            notes: ç”¨æˆ·çš„æ‰€æœ‰ç¬”è®°åˆ—è¡¨
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        if not notes:
            return ""
        
        # è·å–ç”¨æˆ·ä¿¡æ¯
        first_note = notes[0]
        # å°è¯•å¤šç§æ–¹å¼è·å–æ˜µç§°
        nickname = (first_note.get('nickname') or 
                   first_note.get('user', {}).get('nickname') or 
                   'unknown')
        safe_nickname = norm_str(nickname)[:20]
        if not safe_nickname.strip():
            safe_nickname = 'unknown'
        
        # åˆ›å»ºç”¨æˆ·ç›®å½•
        user_dir = os.path.join(self.save_dir, f"{safe_nickname}_{user_id}")
        os.makedirs(user_dir, exist_ok=True)
        
        # ç”Ÿæˆæ±‡æ€»æ•°æ®
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        summary = {
            'user_id': user_id,
            'nickname': nickname,
            'crawl_time': timestamp,
            'total_notes': len(notes),
            'notes': notes
        }
        
        # ä¿å­˜æ±‡æ€»æ–‡ä»¶
        filename = f"user_{user_id}_summary_{timestamp}.json"
        filepath = os.path.join(user_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ç”¨æˆ·ç¬”è®°æ±‡æ€»å·²ä¿å­˜åˆ°: {filepath}")
        return filepath
