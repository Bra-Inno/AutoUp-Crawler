# hotlist_crawler package
"""
é€šç”¨çƒ­æ¦œå†…å®¹çˆ¬è™«åŒ…
æ”¯æŒçŸ¥ä¹ã€å¾®åšã€å¾®ä¿¡å…¬ä¼—å·ç­‰å¹³å°çš„å†…å®¹æŠ“å–
"""

import sys
import os
import asyncio
from typing import Optional, List, Dict, Any

# ç‰ˆæœ¬ä¿¡æ¯
__version__ = "2.0.0"
__author__ = "hotlist-crawler"
__email__ = ""
__description__ = "é€šç”¨çƒ­æ¦œå†…å®¹çˆ¬è™«åŒ…ï¼Œæ”¯æŒçŸ¥ä¹ã€å¾®åšã€å¾®ä¿¡å…¬ä¼—å·ç­‰å¹³å°çš„å†…å®¹æŠ“å–"

# Windows ç³»ç»Ÿå¼‚æ­¥æ”¯æŒ
if sys.platform == "win32":
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„ï¼Œç¡®ä¿å¯ä»¥å¯¼å…¥æ¨¡å—
_current_dir = os.path.dirname(os.path.abspath(__file__))
_parent_dir = os.path.dirname(_current_dir)
if _parent_dir not in sys.path:
    sys.path.insert(0, _parent_dir)

try:
    # å¯¼å…¥æ ¸å¿ƒåŠŸèƒ½
    from .core import scrape, scrape_zhihu, scrape_weibo, scrape_weixin, identify_platform
    from .auth import login, is_online, login_sync, get_all_online_status
    from .fetch import fetch, batch_fetch, validate_destination, list_supported_platforms, get_platform_info
    from .types import PlatformType, USER_DATA_DIR
    from .models import ScrapedDataItem
    
    # æˆåŠŸå¯¼å…¥æ ‡å¿—
    _import_success = True
    _import_error = None
    
except ImportError as e:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ç»å¯¹å¯¼å…¥
    try:
        import hotlist_crawler.core as core
        import hotlist_crawler.auth as auth_module
        import hotlist_crawler.fetch as fetch_module
        import hotlist_crawler.models as models
        import hotlist_crawler.types as types_module
        
        # å¯¼å…¥å‡½æ•°
        scrape = core.scrape
        scrape_zhihu = core.scrape_zhihu
        scrape_weibo = core.scrape_weibo
        scrape_weixin = core.scrape_weixin
        identify_platform = core.identify_platform
        
        login = auth_module.login
        is_online = auth_module.is_online
        login_sync = auth_module.login_sync
        get_all_online_status = auth_module.get_all_online_status
        
        fetch = fetch_module.fetch
        batch_fetch = fetch_module.batch_fetch
        validate_destination = fetch_module.validate_destination
        list_supported_platforms = fetch_module.list_supported_platforms
        get_platform_info = fetch_module.get_platform_info
        
        PlatformType = types_module.PlatformType
        USER_DATA_DIR = types_module.USER_DATA_DIR
        
        ScrapedDataItem = models.ScrapedDataItem
        
        _import_success = True
        _import_error = None
        
    except ImportError as e2:
        # å¯¼å…¥å¤±è´¥ï¼Œè®°å½•é”™è¯¯ä½†ä¸å´©æºƒ
        _import_success = False
        _import_error = str(e2)
        
        # æä¾›é”™è¯¯æç¤ºå‡½æ•°
        def _import_error_func(*args, **kwargs):
            raise ImportError(f"hotlist_crawleræ¨¡å—å¯¼å…¥å¤±è´¥: {_import_error}")
        
        # ç”¨é”™è¯¯å‡½æ•°æ›¿ä»£ä¸»è¦åŠŸèƒ½
        scrape = _import_error_func
        scrape_zhihu = _import_error_func
        scrape_weibo = _import_error_func
        scrape_weixin = _import_error_func
        identify_platform = _import_error_func
        login = _import_error_func
        is_online = _import_error_func
        login_sync = _import_error_func
        get_all_online_status = _import_error_func
        fetch = _import_error_func
        batch_fetch = _import_error_func
        validate_destination = _import_error_func
        list_supported_platforms = _import_error_func
        get_platform_info = _import_error_func
        PlatformType = None
        USER_DATA_DIR = os.path.join(os.getcwd(), "chrome_user_data")  # å›ºå®šä¿å­˜åˆ°å·¥ä½œç›®å½•
        ScrapedDataItem = None

# å¯¼å‡ºçš„å…¬å…±æ¥å£
__all__ = [
    # æ ¸å¿ƒçˆ¬è™«åŠŸèƒ½
    'scrape',           # é€šç”¨çˆ¬å–å‡½æ•°
    'scrape_zhihu',     # çŸ¥ä¹ä¸“ç”¨çˆ¬å–
    'scrape_weibo',     # å¾®åšä¸“ç”¨çˆ¬å–
    'scrape_weixin',    # å¾®ä¿¡ä¸“ç”¨çˆ¬å–
    'identify_platform', # å¹³å°è¯†åˆ«
    
    # FetchåŠŸèƒ½ - ç¬¦åˆAPIè®¾è®¡
    'fetch',            # æ ¸å¿ƒfetchå‡½æ•° -> bool
    'batch_fetch',      # æ‰¹é‡æŠ“å–
    'validate_destination', # éªŒè¯ç›®æ ‡ç›®å½•
    'list_supported_platforms', # è·å–æ”¯æŒçš„å¹³å°
    'get_platform_info', # è·å–å¹³å°ä¿¡æ¯
    
    # ç™»å½•åŠŸèƒ½ - ç¬¦åˆAPIè®¾è®¡
    'login',            # ç™»å½•å‡½æ•° -> bool
    'is_online',        # æ£€æŸ¥åœ¨çº¿çŠ¶æ€ -> bool
    'login_sync',       # åŒæ­¥ç‰ˆç™»å½•
    'get_all_online_status', # è·å–æ‰€æœ‰å¹³å°çŠ¶æ€
    
    # ç±»å‹å’Œå¸¸é‡
    'PlatformType',     # å¹³å°ç±»å‹æšä¸¾
    'USER_DATA_DIR',    # ç”¨æˆ·æ•°æ®ç›®å½•
    
    # æ•°æ®æ¨¡å‹
    'ScrapedDataItem',  # æ•°æ®é¡¹æ¨¡å‹
    
    # ä¾¿æ·åˆ«å
    'zhihu',           # scrape_zhihu çš„åˆ«å
    'weibo',           # scrape_weibo çš„åˆ«å
    'weixin',          # scrape_weixin çš„åˆ«å
    
    # ç‰ˆæœ¬ä¿¡æ¯
    '__version__',
]

# åˆ›å»ºä¾¿æ·åˆ«å
def _create_aliases():
    """åˆ›å»ºä¾¿æ·åˆ«å"""
    global zhihu, weibo, weixin
    if _import_success:
        zhihu = scrape_zhihu
        weibo = scrape_weibo
        weixin = scrape_weixin
    else:
        def _error_func(*args, **kwargs):
            raise ImportError(f"hotlist_crawleræ¨¡å—å¯¼å…¥å¤±è´¥: {_import_error}")
        zhihu = _error_func
        weibo = _error_func
        weixin = _error_func

# æ‰§è¡Œåˆ«ååˆ›å»º
_create_aliases()


def get_version() -> str:
    """è·å–åŒ…ç‰ˆæœ¬å·"""
    return __version__


def get_supported_platforms() -> List[str]:
    """è·å–æ”¯æŒçš„å¹³å°åˆ—è¡¨"""
    return ["zhihu", "weibo", "weixin_mp", "xiaohongshu", "douyin", "bilibili"]


def health_check() -> Dict[str, Any]:
    """å¥åº·æ£€æŸ¥ï¼Œè¿”å›åŒ…çš„çŠ¶æ€ä¿¡æ¯"""
    return {
        "package": "hotlist_crawler",
        "version": __version__,
        "import_success": _import_success,
        "import_error": _import_error,
        "supported_platforms": get_supported_platforms(),
        "python_version": sys.version,
        "platform": sys.platform
    }


# åŒ…åˆå§‹åŒ–æ—¶çš„æç¤ºä¿¡æ¯
if _import_success:
    # æˆåŠŸå¯¼å…¥æ—¶çš„é™é»˜æ¨¡å¼ï¼Œé¿å…è¿‡å¤šè¾“å‡º
    pass
else:
    print(f"âš ï¸ hotlist_crawleråŒ…å¯¼å…¥æ—¶é‡åˆ°é—®é¢˜: {_import_error}")
    print("ğŸ’¡ è¯·ç¡®ä¿æ‰€æœ‰ä¾èµ–å·²æ­£ç¡®å®‰è£…ï¼Œæˆ–æŸ¥çœ‹æ–‡æ¡£è·å–å¸®åŠ©")


# ä¾¿æ·çš„ä½¿ç”¨ç¤ºä¾‹ï¼ˆåœ¨æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ï¼‰
"""
ä½¿ç”¨ç¤ºä¾‹:

# åŸºæœ¬çˆ¬å–åŠŸèƒ½
import hotlist_crawler

# è‡ªåŠ¨è¯†åˆ«å¹³å°å¹¶çˆ¬å–
result = hotlist_crawler.scrape("https://www.zhihu.com/question/12345")

# å¹³å°ä¸“ç”¨çˆ¬å–
zhihu_data = hotlist_crawler.scrape_zhihu("https://www.zhihu.com/question/12345", max_answers=5)
weibo_data = hotlist_crawler.scrape_weibo("https://s.weibo.com/weibo?q=Python")

# ä½¿ç”¨åˆ«å
result = hotlist_crawler.zhihu("https://www.zhihu.com/question/12345")

# ç™»å½•åŠŸèƒ½
import asyncio
login_result = asyncio.run(hotlist_crawler.login("zhihu"))

# è·å–ç™»å½•çŠ¶æ€
cookies = hotlist_crawler.get_cookies("zhihu")
status = hotlist_crawler.get_login_status("zhihu")

# å¥åº·æ£€æŸ¥
health = hotlist_crawler.health_check()
print(health)
"""