"""
é€šç”¨çƒ­æ¦œå†…å®¹çˆ¬è™«åŒ…
æ”¯æŒçŸ¥ä¹ã€å¾®åšã€å¾®ä¿¡å…¬ä¼—å·ç­‰å¹³å°çš„å†…å®¹æŠ“å–
"""

import sys
import os
import asyncio
from typing import List, Dict, Any
from loguru import logger

# ç‰ˆæœ¬ä¿¡æ¯
__version__ = "2.0.0"
__author__ = "hotlist-crawler"
__email__ = ""
__description__ = "é€šç”¨çƒ­æ¦œå†…å®¹çˆ¬è™«åŒ…ï¼Œæ”¯æŒçŸ¥ä¹ã€å¾®åšã€å¾®ä¿¡å…¬ä¼—å·ç­‰å¹³å°çš„å†…å®¹æŠ“å–"

# Windows ç³»ç»Ÿå¼‚æ­¥æ”¯æŒ
if sys.platform == "win32":
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„ï¼Œç¡®ä¿å¯ä»¥å¯¼å…¥æ¨¡å—
_current_dir = os.path.dirname(os.path.abspath(__file__))
_parent_dir = os.path.dirname(_current_dir)
if _parent_dir not in sys.path:
    sys.path.insert(0, _parent_dir)

try:
    # å¯¼å…¥æ ¸å¿ƒåŠŸèƒ½
    from .core import (
        scrape,
        scrape_zhihu,
        scrape_weibo,
        scrape_weixin,
        scrape_bilibili,
        scrape_douyin,
        identify_platform,
    )
    from .auth import login, is_online, get_all_online_status
    from .fetch import (
        fetch,
        batch_fetch,
        validate_destination,
        list_supported_platforms,
        get_platform_info,
    )
    from .types import PlatformType
    from .models import ScrapedDataItem

    # å¯¼å…¥é…ç½®ç®¡ç†åŠŸèƒ½
    from .config import (
        set_user_data_dir,
        set_user_agent,
        get_user_data_dir,
        get_user_agent,
        get_login_data_dir,
        set_playwright_headless,
        set_download_dir,
        get_all_config,
        print_config,
    )

    # æˆåŠŸå¯¼å…¥æ ‡å¿—
    _import_success = True
    _import_error = None

except ImportError as e:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ç»å¯¹å¯¼å…¥
    try:
        import hotlist_crawler.core as core
        import hotlist_crawler.auth as auth_module
        import hotlist_crawler.fetch as fetch_module
        import hotlist_crawler.models as models
        import hotlist_crawler.types as types_module

        # å¯¼å…¥å‡½æ•°
        scrape = core.scrape
        scrape_zhihu = core.scrape_zhihu
        scrape_weibo = core.scrape_weibo
        scrape_weixin = core.scrape_weixin
        scrape_bilibili = core.scrape_bilibili
        scrape_douyin = core.scrape_douyin
        identify_platform = core.identify_platform

        login = auth_module.login
        is_online = auth_module.is_online
        get_all_online_status = auth_module.get_all_online_status

        fetch = fetch_module.fetch
        batch_fetch = fetch_module.batch_fetch
        validate_destination = fetch_module.validate_destination
        list_supported_platforms = fetch_module.list_supported_platforms
        get_platform_info = fetch_module.get_platform_info

        PlatformType = types_module.PlatformType

        ScrapedDataItem = models.ScrapedDataItem

        _import_success = True
        _import_error = None

    except ImportError as e2:
        # å¯¼å…¥å¤±è´¥ï¼Œè®°å½•é”™è¯¯ä½†ä¸å´©æºƒ
        _import_success = False
        _import_error = str(e2)

        # æä¾›é”™è¯¯æç¤ºå‡½æ•°
        def _import_error_func(*args, **kwargs):
            raise ImportError(f"hotlist_crawleræ¨¡å—å¯¼å…¥å¤±è´¥: {_import_error}")

        # ç”¨é”™è¯¯å‡½æ•°æ›¿ä»£ä¸»è¦åŠŸèƒ½
        scrape = _import_error_func
        scrape_zhihu = _import_error_func
        scrape_weibo = _import_error_func
        scrape_weixin = _import_error_func
        scrape_bilibili = _import_error_func
        scrape_douyin = _import_error_func
        identify_platform = _import_error_func
        login = _import_error_func
        is_online = _import_error_func
        get_all_online_status = _import_error_func
        fetch = _import_error_func
        batch_fetch = _import_error_func
        validate_destination = _import_error_func
        list_supported_platforms = _import_error_func
        get_platform_info = _import_error_func

        # é…ç½®ç®¡ç†å‡½æ•°
        set_user_data_dir = _import_error_func
        set_user_agent = _import_error_func
        set_login_data_dir = _import_error_func
        get_user_data_dir = _import_error_func
        get_user_agent = _import_error_func
        get_login_data_dir = _import_error_func
        set_playwright_headless = _import_error_func
        set_download_dir = _import_error_func
        get_all_config = _import_error_func
        print_config = _import_error_func

        PlatformType = None
        ScrapedDataItem = None

# å¯¼å‡ºçš„å…¬å…±æ¥å£
__all__ = [
    # æ ¸å¿ƒçˆ¬è™«åŠŸèƒ½
    "scrape",  # é€šç”¨çˆ¬å–å‡½æ•°
    "scrape_zhihu",  # çŸ¥ä¹ä¸“ç”¨çˆ¬å–
    "scrape_weibo",  # å¾®åšä¸“ç”¨çˆ¬å–
    "scrape_weixin",  # å¾®ä¿¡ä¸“ç”¨çˆ¬å–
    "scrape_bilibili",  # Bç«™ä¸“ç”¨çˆ¬å–
    "scrape_douyin",  # æŠ–éŸ³ä¸“ç”¨çˆ¬å–
    "identify_platform",  # å¹³å°è¯†åˆ«
    # FetchåŠŸèƒ½ - ç¬¦åˆAPIè®¾è®¡
    "fetch",  # æ ¸å¿ƒfetchå‡½æ•° -> bool
    "batch_fetch",  # æ‰¹é‡æŠ“å–
    "validate_destination",  # éªŒè¯ç›®æ ‡ç›®å½•
    "list_supported_platforms",  # è·å–æ”¯æŒçš„å¹³å°
    "get_platform_info",  # è·å–å¹³å°ä¿¡æ¯
    # ç™»å½•åŠŸèƒ½ - ç¬¦åˆAPIè®¾è®¡
    "login",  # ç™»å½•å‡½æ•° -> bool
    "is_online",  # æ£€æŸ¥åœ¨çº¿çŠ¶æ€ -> bool
    "get_all_online_status",  # è·å–æ‰€æœ‰å¹³å°çŠ¶æ€
    # é…ç½®ç®¡ç†åŠŸèƒ½
    "set_user_data_dir",  # è®¾ç½®æµè§ˆå™¨æ•°æ®ç›®å½•
    "set_user_agent",  # è®¾ç½®User-Agent
    "set_login_data_dir",  # è®¾ç½®ç™»å½•æ•°æ®ç›®å½•
    "get_user_data_dir",  # è·å–æµè§ˆå™¨æ•°æ®ç›®å½•
    "get_user_agent",  # è·å–User-Agent
    "get_login_data_dir",  # è·å–ç™»å½•æ•°æ®ç›®å½•
    "set_playwright_headless",  # è®¾ç½®æ— å¤´æ¨¡å¼
    "set_download_dir",  # è®¾ç½®ä¸‹è½½ç›®å½•
    "get_all_config",  # è·å–æ‰€æœ‰é…ç½®
    "print_config",  # æ‰“å°é…ç½®ä¿¡æ¯
    # ç±»å‹å’Œå¸¸é‡
    "PlatformType",  # å¹³å°ç±»å‹æšä¸¾
    # æ•°æ®æ¨¡å‹
    "ScrapedDataItem",  # æ•°æ®é¡¹æ¨¡å‹
    # ä¾¿æ·åˆ«å
    "zhihu",
    "weibo",
    "weixin",
    "bilibili",
    "douyin",
    # ç‰ˆæœ¬ä¿¡æ¯
    "__version__",
]


if _import_success:
    zhihu = scrape_zhihu
    weibo = scrape_weibo
    weixin = scrape_weixin
    bilibili = scrape_bilibili
    douyin = scrape_douyin
else:

    def _error_func(*args, **kwargs):
        raise ImportError(f"hotlist_crawleræ¨¡å—å¯¼å…¥å¤±è´¥: {_import_error}")

    zhihu = _error_func
    weibo = _error_func
    weixin = _error_func
    bilibili = _error_func
    douyin = _error_func


def get_supported_platforms() -> List[str]:
    """è·å–æ”¯æŒçš„å¹³å°åˆ—è¡¨"""
    return ["zhihu", "weibo", "weixin_mp", "xiaohongshu", "douyin", "bilibili"]


def health_check() -> Dict[str, Any]:
    """å¥åº·æ£€æŸ¥ï¼Œè¿”å›åŒ…çš„çŠ¶æ€ä¿¡æ¯"""
    return {
        "package": "hotlist_crawler",
        "version": __version__,
        "import_success": _import_success,
        "import_error": _import_error,
        "supported_platforms": get_supported_platforms(),
        "python_version": sys.version,
        "platform": sys.platform,
    }


# åŒ…åˆå§‹åŒ–æ—¶çš„æç¤ºä¿¡æ¯
if not _import_success:
    logger.warning(f"âš ï¸ hotlist_crawleråŒ…å¯¼å…¥æ—¶é‡åˆ°é—®é¢˜: {_import_error}")
    logger.info("ğŸ’¡ è¯·ç¡®ä¿æ‰€æœ‰ä¾èµ–å·²æ­£ç¡®å®‰è£…ï¼Œæˆ–æŸ¥çœ‹æ–‡æ¡£è·å–å¸®åŠ©")
