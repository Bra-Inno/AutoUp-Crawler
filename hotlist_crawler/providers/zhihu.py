import os
import re
import json
import asyncio
import httpx
from typing import Any, List, Dict
from concurrent.futures import ThreadPoolExecutor
from bs4 import BeautifulSoup
from loguru import logger

from ..models import ImageInfo
from ..providers.base import BaseProvider
from ..models import ScrapedDataItem
from ..storage import storage_manager
from ..file_utils import get_file_extension, get_random_user_agent


class ZhihuArticleProvider(BaseProvider):
    """
    çŸ¥ä¹æ–‡ç« å’Œé—®é¢˜é¡µé¢çš„çˆ¬è™«å®ç°
    """

    def __init__(
        self,
        url: str,
        rules: dict,
        save_images: bool = True,
        output_format: str = "markdown",
        cookies: list | None = None,
        force_save: bool = True,
        max_answers: int = 3,
    ):
        super().__init__(url, rules, save_images, output_format, force_save, "zhihu")
        self.max_answers = max_answers
        self.cookies = cookies

    async def fetch_and_parse(self) -> Any:
        if "www.zhihu.com/question" in self.url:
            return await self._parse_question_page()
        logger.error("âŒ ä»…æ”¯æŒçŸ¥ä¹é—®é¢˜é¡µé¢çš„è§£æ")
        raise ValueError("Only Zhihu question pages are supported.")

    async def _parse_question_page(self) -> Any:
        """ä½¿ç”¨Playwrightè§£æçŸ¥ä¹é—®é¢˜é¡µé¢ï¼ˆæ•´åˆtest.pyé€»è¾‘ï¼‰"""

        def _sync_playwright_parse():
            """åŒæ­¥Playwrightè§£æå‡½æ•°"""
            from playwright.sync_api import sync_playwright

            with sync_playwright() as playwright:

                # 1. å¯åŠ¨ä¸€ä¸ª "å¹²å‡€" çš„æµè§ˆå™¨
                browser = playwright.chromium.launch(
                    headless=True,
                    slow_mo=100,
                    ignore_default_args=["--enable-automation"],
                    args=["--disable-blink-features=AutomationControlled"],
                )

                # 2. åˆ›å»ºä¸€ä¸ªæ–°ä¸Šä¸‹æ–‡
                context = browser.new_context(user_agent=get_random_user_agent("chrome"))

                # 3. æ‰‹åŠ¨æ³¨å…¥ cookies
                saved_cookies = self.cookies
                if saved_cookies:
                    context.add_cookies(saved_cookies)
                    logger.info("âœ… çŸ¥ä¹ç™»å½•çŠ¶æ€å·²åŠ è½½")
                else:
                    logger.warning("âš ï¸ æœªæ‰¾åˆ° self.cookiesï¼Œå°†ä»¥æœªç™»å½•çŠ¶æ€å¯åŠ¨ã€‚")

                page = context.new_page()
                try:
                    logger.debug(f"ğŸŒ æ­£åœ¨è®¿é—®çŸ¥ä¹é—®é¢˜é¡µé¢: {self.url}")

                    # è®¿é—®é¡µé¢
                    page.goto(self.url, timeout=90000, wait_until="networkidle")
                    page.wait_for_selector("h1.QuestionHeader-title", timeout=60000)
                    logger.info("âœ… é¡µé¢å·²ç¨³å®šï¼")

                    # æ£€æŸ¥æ˜¯å¦éœ€è¦ç™»å½•
                    if page.is_visible('button.Button--primary.Button--blue:has-text("ç™»å½•")'):
                        logger.debug("\nğŸ” éœ€è¦ç™»å½•çŸ¥ä¹è´¦å·ï¼Œè¯·åœ¨æµè§ˆå™¨ä¸­æ‰‹åŠ¨ç™»å½•...")
                        try:
                            page.wait_for_selector("div.AppHeader-profile", timeout=120000)
                            logger.info("âœ… ç™»å½•æˆåŠŸï¼")
                        except:
                            logger.warning("âš ï¸ ç™»å½•è¶…æ—¶ï¼Œå°è¯•ç»§ç»­...")

                    # ç‚¹å‡»"æ˜¾ç¤ºå…¨éƒ¨"æŒ‰é’®ä»¥å±•å¼€é—®é¢˜æè¿°
                    logger.debug("\nğŸ“– æ­£åœ¨æ£€æŸ¥é—®é¢˜æè¿°æ˜¯å¦éœ€è¦å±•å¼€...")
                    show_all_button_selector = "button.QuestionRichText-more"
                    if page.is_visible(show_all_button_selector):
                        try:
                            page.click(show_all_button_selector, timeout=5000)
                            logger.debug("  - æˆåŠŸç‚¹å‡» 'æ˜¾ç¤ºå…¨éƒ¨' æŒ‰é’®ï¼Œç­‰å¾…å†…å®¹åŠ è½½...")
                            page.wait_for_timeout(2000)
                        except Exception as e:
                            logger.warning(f"  - ç‚¹å‡» 'æ˜¾ç¤ºå…¨éƒ¨' æŒ‰é’®å¤±è´¥: {e}")
                    else:
                        logger.debug("  - æ— éœ€å±•å¼€ï¼Œé—®é¢˜æè¿°å·²æ˜¯å…¨æ–‡ã€‚")

                    # ç­‰å¾…é¡µé¢ç¨³å®š
                    logger.debug(f"ğŸ“ ä»…å¤„ç†é¡µé¢å‰ {self.max_answers} ä¸ªå›ç­”ã€‚")
                    page.wait_for_timeout(1000)

                    # è·å–é¡µé¢å†…å®¹
                    final_html = page.content()
                    soup = BeautifulSoup(final_html, "html.parser")

                    # æå–é—®é¢˜ä¿¡æ¯
                    question_title_element = soup.find("h1", class_="QuestionHeader-title")
                    question_title = (
                        question_title_element.get_text(strip=True) if question_title_element else "æœªçŸ¥é—®é¢˜æ ‡é¢˜"
                    )

                    question_detail_element = soup.find("div", class_="QuestionRichText")
                    question_detail = (
                        question_detail_element.get_text("\n", strip=True) if question_detail_element else ""
                    )

                    logger.info(f"ğŸ“‹ é—®é¢˜æ ‡é¢˜: {question_title}")

                    # åˆ›å»ºå­˜å‚¨ç»“æ„
                    storage_info = None
                    question_images = []
                    if self.force_save:
                        storage_info = storage_manager.create_article_storage(
                            platform=self.platform_name,
                            title=question_title,
                            url=self.url,
                        )

                        # ä¸‹è½½é—®é¢˜æè¿°ä¸­çš„å›¾ç‰‡
                        if question_detail_element and self.save_images:
                            question_images = self._sync_download_question_images(question_detail_element, storage_info)

                    # è§£æå›ç­”
                    answers_list = []
                    downloaded_images = []
                    answer_items = soup.find_all("div", class_="AnswerItem")

                    logger.debug(f"ğŸ“Š é¡µé¢å…±åŠ è½½äº† {len(answer_items)} ä¸ªå›ç­”ï¼Œå°†å¤„ç†å‰ {self.max_answers} ä¸ªã€‚")

                    for index, item in enumerate(answer_items[: self.max_answers]):
                        # ä½¿ç”¨å­—ç¬¦ä¸²æ“ä½œæ¥æå–ä¿¡æ¯ï¼Œé¿å…ç±»å‹é—®é¢˜
                        item_html = str(item)

                        # è·å–ä½œè€…ä¿¡æ¯
                        author_match = re.search(r'<meta[^>]*itemprop="name"[^>]*content="([^"]*)"', item_html)
                        if author_match:
                            author = author_match.group(1)
                        else:
                            # å°è¯•ä»ç”¨æˆ·é“¾æ¥è·å–
                            author_match = re.search(r'class="UserLink-link"[^>]*>([^<]+)<', item_html)
                            author = author_match.group(1).strip() if author_match else "åŒ¿åç”¨æˆ·"

                        # è·å–ç‚¹èµæ•°
                        vote_match = re.search(r'<meta[^>]*itemprop="upvoteCount"[^>]*content="([^"]*)"', item_html)
                        if vote_match:
                            try:
                                upvotes = int(vote_match.group(1))
                            except (ValueError, TypeError):
                                upvotes = 0
                        else:
                            upvotes = 0

                        # è·å–å›ç­”å†…å®¹ï¼ˆä½¿ç”¨BeautifulSoupï¼Œä½†åŠ ç±»å‹å¿½ç•¥ï¼‰
                        content_element = item.find("div", {"class": "RichContent-inner"})  # type: ignore
                        if content_element:
                            content_text = content_element.get_text(separator="\n", strip=True)  # type: ignore
                        else:
                            content_text = ""

                        # å¤„ç†å›¾ç‰‡
                        answer_images = []
                        if content_element and self.save_images and storage_info:
                            answer_images = self._sync_download_answer_images(
                                content_element, storage_info, index, author
                            )
                            downloaded_images.extend(answer_images)

                        answers_list.append(
                            {
                                "author": author,
                                "upvotes": upvotes,
                                "content": content_text,
                                "images": answer_images,
                            }
                        )

                        logger.debug(f"  âœ… å¤„ç†å®Œæˆç¬¬ {index + 1} ä¸ªå›ç­” (ä½œè€…: {author}, ğŸ‘ {upvotes})")

                    # ç»„è£…å®Œæ•´å†…å®¹
                    full_content = f"# {question_title}\n\n"
                    if question_detail:
                        full_content += f"## é—®é¢˜æè¿°\n\n{question_detail}\n\n"

                    full_content += f"## å›ç­” ({len(answers_list)}ä¸ª)\n\n"
                    for i, answer in enumerate(answers_list, 1):
                        full_content += f"### å›ç­” {i} - {answer['author']} (ğŸ‘ {answer['upvotes']})\n\n"
                        full_content += f"{answer['content']}\n\n"

                    # ä¿å­˜å†…å®¹
                    if storage_info:
                        storage_manager.save_text_content(storage_info, full_content)

                        if self.output_format == "markdown":
                            markdown_content = self._convert_to_markdown(question_title, question_detail, answers_list)
                            storage_manager.save_markdown_content(storage_info, markdown_content, question_title)

                        # ä¿å­˜å®Œæ•´çš„JSONæ•°æ®
                        json_data = {
                            "question_url": self.url,
                            "question_title": question_title,
                            "question_detail": question_detail,
                            "question_images": question_images,
                            "answers_count": len(answers_list),
                            "answers": answers_list,
                        }

                        json_path = os.path.join(storage_info["article_dir"], "data.json")
                        with open(json_path, "w", encoding="utf-8") as f:
                            json.dump(json_data, f, ensure_ascii=False, indent=4)

                        storage_manager.save_article_index(storage_info, full_content[:200])

                        logger.info(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {storage_info['article_dir']}")

                    all_image_infos = []
                    for img_path in question_images + downloaded_images:
                        all_image_infos.append(
                            ImageInfo(
                                original_url="local_file",
                                local_path=img_path,
                                alt_text=os.path.basename(img_path),
                            )
                        )

                    return ScrapedDataItem(
                        title=question_title,
                        content=full_content,
                        author="çŸ¥ä¹ç”¨æˆ·",
                        images=all_image_infos,
                        markdown_content=(
                            self._convert_to_markdown(question_title, question_detail, answers_list)
                            if self.output_format == "markdown"
                            else None
                        ),
                        save_directory=(storage_info["article_dir"] if storage_info else None),
                    )

                except Exception as e:
                    logger.error(f"âŒ çŸ¥ä¹é—®é¢˜é¡µé¢è§£æå¤±è´¥: {e}")
                    return None
                finally:
                    context.close()

        # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåŒæ­¥ä»£ç 
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor() as executor:
            return await loop.run_in_executor(executor, _sync_playwright_parse)

    def _sync_download_question_images(self, question_element, storage_info: dict) -> List[str]:
        """ä¸‹è½½é—®é¢˜æè¿°ä¸­çš„å›¾ç‰‡"""
        downloaded_images = []
        question_image_dir = os.path.join(storage_info["images_dir"], "question_images")
        os.makedirs(question_image_dir, exist_ok=True)

        images = question_element.find_all("img")
        logger.debug(f"ğŸ–¼ï¸ æ­£åœ¨å¤„ç†... å‘ç° {len(images)} å¼ ")

        for img_index, img_tag in enumerate(images):
            img_url = img_tag.get("data-original") or img_tag.get("data-actualsrc") or img_tag.get("src")

            if not img_url or not img_url.startswith("http"):
                continue

            try:
                response = httpx.get(img_url, timeout=15)
                response.raise_for_status()

                # æ™ºèƒ½æ£€æµ‹å›¾ç‰‡æ ¼å¼
                content = response.content
                ext = get_file_extension(content)

                img_filename = f"question_image_{img_index + 1}.{ext}"
                local_img_path = os.path.join(question_image_dir, img_filename)

                with open(local_img_path, "wb") as f:
                    f.write(content)

                downloaded_images.append(local_img_path)
                logger.debug(f" {local_img_path}")

            except Exception as e:
                logger.error(f"  - âŒ ä¸‹è½½é—®é¢˜å›¾ç‰‡å¤±è´¥: {img_url}, é”™è¯¯: {e}")

        return downloaded_images

    def _sync_download_answer_images(
        self, content_element, storage_info: dict, answer_index: int, author: str
    ) -> List[str]:
        """ä¸‹è½½å›ç­”ä¸­çš„å›¾ç‰‡"""
        downloaded_images = []

        # åˆ›å»ºå›ç­”ä¸“ç”¨å›¾ç‰‡ç›®å½•
        safe_author = re.sub(r'[\\/:*?"<>|]', "_", author)
        answer_image_dir = os.path.join(storage_info["images_dir"], f"answer_{answer_index + 1}_{safe_author}")
        os.makedirs(answer_image_dir, exist_ok=True)

        images = content_element.find_all("img")
        logger.debug(f"  ğŸ–¼ï¸ æ­£åœ¨å¤„ç†ç¬¬ {answer_index + 1} ä¸ªå›ç­”ä¸­çš„å›¾ç‰‡ï¼Œå‘ç° {len(images)} å¼ ")

        for img_index, img_tag in enumerate(images):
            img_url = img_tag.get("data-original") or img_tag.get("data-actualsrc") or img_tag.get("src")

            if not img_url or not img_url.startswith("http"):
                continue

            try:
                response = httpx.get(img_url, timeout=10)
                response.raise_for_status()

                # è·å–æ­£ç¡®çš„æ–‡ä»¶æ‰©å±•å
                content = response.content
                ext = get_file_extension(content=content)

                img_filename = f"{img_index + 1}.{ext}"
                local_img_path = os.path.join(answer_image_dir, img_filename)

                with open(local_img_path, "wb") as f:
                    f.write(content)

                downloaded_images.append(local_img_path)
                logger.debug(f" {local_img_path}")

            except Exception as e:
                logger.error(f"    - âŒ ä¸‹è½½å›¾ç‰‡å¤±è´¥: {img_url}, é”™è¯¯: {e}")

        return downloaded_images

    def _convert_to_markdown(self, question_title: str, question_detail: str, answers_list: List[Dict]) -> str:
        """å°†çŸ¥ä¹é—®é¢˜å’Œå›ç­”è½¬æ¢ä¸ºMarkdownæ ¼å¼"""
        markdown = f"# {question_title}\n\n"

        if question_detail:
            markdown += f"## é—®é¢˜æè¿°\n\n{question_detail}\n\n"

        markdown += f"## å›ç­” ({len(answers_list)}ä¸ª)\n\n"

        for i, answer in enumerate(answers_list, 1):
            markdown += f"### å›ç­” {i} - {answer['author']} (ğŸ‘ {answer['upvotes']})\n\n"
            markdown += f"{answer['content']}\n\n"

            if answer["images"]:
                markdown += "**ç›¸å…³å›¾ç‰‡:**\n\n"
                for img_path in answer["images"]:
                    img_name = os.path.basename(img_path)
                    # ä½¿ç”¨ç›¸å¯¹è·¯å¾„åœ¨Markdownä¸­å¼•ç”¨å›¾ç‰‡
                    relative_path = f"images/{img_name}"
                    markdown += f"![{img_name}]({relative_path})\n\n"

        return markdown
