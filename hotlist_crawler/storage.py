"""
å†…å®¹å­˜å‚¨ç®¡ç†å™¨
æ”¯æŒä¸åŒå¹³å°çš„åˆ†ç±»å­˜å‚¨ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€å›¾ç‰‡å’Œå…ƒæ•°æ®
"""

import os
import re
import json
from datetime import datetime
from hashlib import md5
from typing import Dict, Optional, TYPE_CHECKING
from loguru import logger

from .utils.file_utils import clean_filename, ensure_directory, get_file_extension

if TYPE_CHECKING:
    from .config import CrawlerConfig


class StorageManager:
    def __init__(self, config: "CrawlerConfig"):
        self.config = config
        self.base_dir = config.download_dir
        self.platform_dirs = {}

    def _get_platform_dir(self, platform: str) -> str:
        """è·å–æˆ–åˆ›å»ºå¹³å°ä¸“ç”¨ç›®å½•"""
        if platform not in self.platform_dirs:
            platform_dir = ensure_directory(os.path.join(self.base_dir, platform))
            self.platform_dirs[platform] = platform_dir
        return self.platform_dirs[platform]

    def _generate_article_id(self, url: str, title: str) -> str:
        """æ ¹æ®URLå’Œæ ‡é¢˜ï¼Œç”Ÿæˆæ–‡ç« çš„å”¯ä¸€æ ‡è¯†ç¬¦"""
        content = f"{url}_{title}"
        return md5(content.encode()).hexdigest()[:12]

    def _update_metadata(self, storage_info: Dict[str, str], updates: Dict, increment: bool = False):
        """æ›´æ–°å…ƒæ•°æ®æ–‡ä»¶"""
        metadata_file = storage_info["metadata_file"]

        if os.path.exists(metadata_file):
            with open(metadata_file, "r", encoding="utf-8") as f:
                metadata = json.load(f)
        else:
            metadata = {}

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        if "statistics" not in metadata:
            metadata["statistics"] = {}

        for key, value in updates.items():
            if increment and key in metadata["statistics"]:
                metadata["statistics"][key] += value
            else:
                metadata["statistics"][key] = value

        metadata["updated_at"] = datetime.now().isoformat()

        # ä¿å­˜æ›´æ–°åçš„å…ƒæ•°æ®
        with open(metadata_file, "w", encoding="utf-8") as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)

    def create_article_storage(self, platform: str, title: str, url: str, author: str | None = None) -> Dict[str, str]:
        """
        ä¸ºæ–‡ç« åˆ›å»ºå­˜å‚¨ç›®å½•ç»“æ„

        è¿”å›åŒ…å«å„ç§è·¯å¾„çš„å­—å…¸
        """
        platform_dir = self._get_platform_dir(platform)
        article_id = self._generate_article_id(url, title)
        safe_title = clean_filename(title, max_length=50)

        # åˆ›å»ºæ–‡ç« ä¸“ç”¨ç›®å½•
        article_dir_name = article_id
        article_dir = ensure_directory(os.path.join(platform_dir, article_dir_name))

        # åˆ›å»ºå­ç›®å½•
        images_dir = ensure_directory(os.path.join(article_dir, "images"))
        attachments_dir = ensure_directory(os.path.join(article_dir, "attachments"))

        # å‡†å¤‡æ–‡ä»¶è·¯å¾„
        text_file = os.path.join(article_dir, "content.txt")
        markdown_file = os.path.join(article_dir, "article.md")
        metadata_file = os.path.join(article_dir, "metadata.json")

        storage_info = {
            "article_id": article_id,
            "article_dir": article_dir,
            "images_dir": images_dir,
            "attachments_dir": attachments_dir,
            "text_file": text_file,
            "markdown_file": markdown_file,
            "metadata_file": metadata_file,
            "platform": platform,
            "title": title,
            "safe_title": safe_title,
        }

        # åˆ›å»ºåˆå§‹å…ƒæ•°æ®
        metadata = {
            "article_id": article_id,
            "title": title,
            "author": author,
            "url": url,
            "platform": platform,
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat(),
            "files": {
                "text_file": os.path.basename(text_file),
                "markdown_file": os.path.basename(markdown_file),
            },
            "statistics": {
                "images_count": 0,
                "attachments_count": 0,
                "content_length": 0,
            },
        }

        # ä¿å­˜å…ƒæ•°æ®
        with open(metadata_file, "w", encoding="utf-8") as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)

        logger.info(f"ğŸ“ åˆ›å»ºå­˜å‚¨ç›®å½•: {article_dir}")
        return storage_info

    def save_text_content(self, storage_info: Dict[str, str], content: str) -> str:
        """ä¿å­˜çº¯æ–‡æœ¬å†…å®¹"""
        text_file = storage_info["text_file"]

        with open(text_file, "w", encoding="utf-8") as f:
            f.write(content)

        logger.debug(f"ğŸ“„ ä¿å­˜æ–‡æœ¬æ–‡ä»¶: {os.path.basename(text_file)}")

        # æ›´æ–°å…ƒæ•°æ®
        self._update_metadata(storage_info, {"content_length": len(content)})

        return text_file

    def save_markdown_content(
        self,
        storage_info: Dict[str, str],
        content: str,
        title: str | None = None,
        author: str | None = None,
    ) -> str:
        """ä¿å­˜Markdownå†…å®¹"""
        markdown_file = storage_info["markdown_file"]

        # æ„å»ºå®Œæ•´çš„Markdownå†…å®¹
        markdown_parts = []

        if title:
            markdown_parts.append(f"# {title}\n")

        if author:
            markdown_parts.append(f"**ä½œè€…:** {author}\n")

        markdown_parts.append("---\n\n")
        markdown_parts.append(content)

        final_content = "".join(markdown_parts)

        # ä¼˜åŒ–å¤šä½™çš„è¿ç»­ç©ºè¡Œå’Œç©ºæ ¼
        final_content = re.sub(r"\n{3,}", "\n\n", final_content)
        final_content = re.sub(r"\s{4,}", "   ", final_content)

        with open(markdown_file, "w", encoding="utf-8") as f:
            f.write(final_content)

        logger.debug(f"ğŸ“„ ä¿å­˜Markdownæ–‡ä»¶: {os.path.basename(markdown_file)}")

        # æ›´æ–°å…ƒæ•°æ®
        self._update_metadata(storage_info, {"markdown_length": len(final_content)})

        return markdown_file

    def save_image(
        self,
        storage_info: Dict[str, str],
        image_data: bytes,
        original_url: str,
        alt_text: str = "",
        image_index: int = 1,
    ) -> Dict[str, str]:
        """
        ä¿å­˜å›¾ç‰‡æ–‡ä»¶ï¼ˆè‡ªåŠ¨è¯†åˆ«æ­£ç¡®æ ¼å¼ï¼‰

        è¿”å›åŒ…å«å›¾ç‰‡ä¿¡æ¯çš„å­—å…¸
        """
        images_dir = storage_info["images_dir"]

        # ä½¿ç”¨å¢å¼ºçš„æ ¼å¼è¯†åˆ«ï¼ˆContent-Type + URL + æ–‡ä»¶ç­¾åï¼‰
        ext = get_file_extension(content=image_data)

        # ç”Ÿæˆæ–‡ä»¶å
        image_filename = f"image_{image_index:03d}.{ext}"
        image_path = os.path.join(images_dir, image_filename)

        # ä¿å­˜å›¾ç‰‡
        with open(image_path, "wb") as f:
            f.write(image_data)

        logger.debug(f"ğŸ–¼ï¸ ä¿å­˜å›¾ç‰‡: {image_filename}")

        # å‡†å¤‡å›¾ç‰‡ä¿¡æ¯
        image_info = {
            "local_path": image_path,
            "relative_path": f"images/{image_filename}",
            "filename": image_filename,
            "original_url": original_url,
            "alt_text": alt_text,
            "size": len(image_data),
        }

        # æ›´æ–°å…ƒæ•°æ®ä¸­çš„å›¾ç‰‡è®¡æ•°
        self._update_metadata(storage_info, {"images_count": 1}, increment=True)

        return image_info

    def save_article_index(self, storage_info: Dict[str, str], content_preview: str = "") -> str:
        """ä¿å­˜æ–‡ç« ç´¢å¼•æ–‡ä»¶ï¼Œæ–¹ä¾¿æµè§ˆ"""
        platform_dir = self._get_platform_dir(storage_info["platform"])
        index_file = os.path.join(platform_dir, "articles_index.json")

        # è¯»å–ç°æœ‰ç´¢å¼•
        if os.path.exists(index_file):
            with open(index_file, "r", encoding="utf-8") as f:
                index_data = json.load(f)
        else:
            index_data = {"articles": [], "last_updated": None}

        # æ·»åŠ æ–°æ–‡ç« åˆ°ç´¢å¼•
        article_entry = {
            "article_id": storage_info["article_id"],
            "title": storage_info["title"],
            "safe_title": storage_info["safe_title"],
            "article_dir": os.path.basename(storage_info["article_dir"]),
            "created_at": datetime.now().isoformat(),
            "preview": (content_preview[:200] + "..." if len(content_preview) > 200 else content_preview),
        }

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™æ›´æ–°
        existing_index = None
        for i, article in enumerate(index_data["articles"]):
            if article["article_id"] == storage_info["article_id"]:
                existing_index = i
                break

        if existing_index is not None:
            index_data["articles"][existing_index] = article_entry
            logger.debug(f"ğŸ“‹ æ›´æ–°æ–‡ç« ç´¢å¼•: {storage_info['title']}")
        else:
            index_data["articles"].append(article_entry)
            logger.debug(f"ğŸ“‹ æ·»åŠ æ–‡ç« åˆ°ç´¢å¼•: {storage_info['title']}")

        index_data["last_updated"] = datetime.now().isoformat()
        index_data["total_articles"] = len(index_data["articles"])

        # ä¿å­˜ç´¢å¼•
        with open(index_file, "w", encoding="utf-8") as f:
            json.dump(index_data, f, ensure_ascii=False, indent=2)

        return index_file
